{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KLD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the Kullback-Leibler (KL) divergence between two Beta distributions, you can use the following formula:\n",
    "\n",
    "KLD(Beta1 || Beta2) = ln(Beta(α2, β2) / Beta(α1, β1)) + (α1 - α2)ψ(α1) + (β1 - β2)ψ(β1) + (α2 - α1 + β2 - β1)ψ(α1 + β1)\n",
    "\n",
    "where:\n",
    "\n",
    "Beta(α, β) is the Beta function.\n",
    "\n",
    "ψ( ) is the digamma function.\n",
    "\n",
    "α1, β1 are the parameters of the first Beta distribution.\n",
    "\n",
    "α2, β2 are the parameters of the second Beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In this code, we use the formula for the Beta distribution probability density function (PDF) to evaluate the PDF for each of the two Beta distributions at 1000 points evenly spaced between 0 and 1. \n",
    "We then set any zero values in the PDFs to a small non-zero value to avoid divide-by-zero errors when calculating the Kullback-Leibler divergence. \n",
    "Finally, we use the numpy.sum() function to calculate the sum of the product of the two PDFs and the logarithm of their ratio, which gives us the Kullback-Leibler divergence between the two distributions.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullback-Leibler divergence: 389.77545312246724\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the parameters for two beta distributions\n",
    "alpha1, beta1 = 2, 5\n",
    "alpha2, beta2 = 3, 4\n",
    "\n",
    "# Generate 1000 points in the range [0, 1] to evaluate the PDF\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Calculate the PDF for the two beta distributions\n",
    "pdf1 = x ** (alpha1 - 1) * (1 - x) ** (beta1 - 1) / \\\n",
    "       (np.math.gamma(alpha1) * np.math.gamma(beta1) / np.math.gamma(alpha1 + beta1))\n",
    "pdf2 = x ** (alpha2 - 1) * (1 - x) ** (beta2 - 1) / \\\n",
    "       (np.math.gamma(alpha2) * np.math.gamma(beta2) / np.math.gamma(alpha2 + beta2))\n",
    "\n",
    "# Set any zero values in the PDFs to a small non-zero value to avoid divide-by-zero errors\n",
    "pdf1[pdf1 == 0] = np.finfo(float).eps\n",
    "pdf2[pdf2 == 0] = np.finfo(float).eps\n",
    "\n",
    "# Calculate the Kullback-Leibler divergence using the numpy.sum() function\n",
    "kld = np.sum(pdf1 * np.log(pdf1 / pdf2))\n",
    "\n",
    "print(\"Kullback-Leibler divergence:\", kld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparmaetrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beta distribution can be reparametrized in terms of its mean and variance. Let's say you have a Beta distribution with parameters α and β. The mean and variance of this Beta distribution are given by:\n",
    "\n",
    "mean = α / (α + β)\n",
    "\n",
    "variance = (α * β) / ((α + β)**2 * (α + β + 1))\n",
    "\n",
    "To reparametrize the Beta distribution in terms of its mean and variance, you can solve the above equations for α and β:\n",
    "\n",
    "α = mean * (mean * (1 - mean) / variance - 1)\n",
    "\n",
    "β = (1 - mean) * (mean * (1 - mean) / variance - 1)\n",
    "\n",
    "Once you have the mean and variance, you can use these equations to compute the new values of α and β, and then use the reparametrized Beta distribution to generate random samples.\n",
    "\n",
    "Here's an example code in Python that shows how to reparametrize the Beta distribution using its mean and variance:\n",
    "\n",
    "Kingma, D. P., & Welling, M. (2014). Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original mean and variance: mean=0.4, variance=0.06\n",
      "New alpha and beta: alpha=1.2000000000000002, beta=1.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reparametrize_beta(mean, variance):\n",
    "    alpha = mean * ((mean * (1 - mean)) / variance - 1)\n",
    "    beta = (1 - mean) * ((mean * (1 - mean)) / variance - 1)\n",
    "    return alpha, beta\n",
    "\n",
    "# Define the original mean and variance of the Beta distribution\n",
    "mean = 0.4\n",
    "variance = 0.06\n",
    "\n",
    "# Reparametrize the Beta distribution using the mean and variance\n",
    "alpha, beta = reparametrize_beta(mean, variance)\n",
    "\n",
    "# Print the original and reparametrized parameters\n",
    "print(f\"Original mean and variance: mean={mean}, variance={variance}\")\n",
    "print(f\"New alpha and beta: alpha={alpha}, beta={beta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The conjugate prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conjugate prior for the Beta distribution is also a Beta distribution. This means that if we assume a Beta prior for the parameter of a Binomial distribution, the posterior distribution after observing data from the Binomial distribution will also be a Beta distribution.\n",
    "\n",
    "More specifically, if we have a Binomial distribution with parameter θ, and we assume a Beta prior with parameters α and β, then the posterior distribution for θ after observing data from the Binomial distribution is a Beta distribution with updated parameters α' = α + x and β' = β + n - x, where x is the number of successes observed and n is the total number of trials.\n",
    "\n",
    "The Beta distribution is a conjugate prior for the Binomial distribution because the product of a Binomial likelihood and a Beta prior results in a Beta posterior. This property is useful because it allows us to update our beliefs about the parameter of the Binomial distribution in a closed form, which can be computationally efficient and mathematically elegant.\n",
    "\n",
    "Conjugate priors are often used in Bayesian statistics because they allow for easy computation of posterior distributions, and they often have intuitive interpretations that make them easy to work with.\n",
    "\n",
    "Reference:\n",
    "\n",
    "Murphy, K. (2012). Machine learning: a probabilistic perspective. MIT press.\n",
    "Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis (Vol. 2). Boca Raton, FL: Chapman & Hall/CRC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In VAE models with Beta-distributed latent variables, the KL divergence (KLD) can be calculated as follows:\n",
    "\n",
    "Let p(z) be the prior distribution on the latent variable z, which is a Beta distribution with parameters (α, β), and let q(z|x) be the approximate posterior distribution of z given the observed data x. We can then compute the KLD between q(z|x) and p(z) as follows:\n",
    "\n",
    "D_KL(q(z|x) || p(z)) = ∫ q(z|x) log(q(z|x) / p(z)) dz\n",
    "\n",
    "We can further simplify this expression as follows:\n",
    "\n",
    "D_KL(q(z|x) || p(z)) = -H(q(z|x)) - ∫ q(z|x) log(p(z)) dz\n",
    "\n",
    "where H(q(z|x)) is the entropy of the posterior distribution, which can be computed analytically for the Beta distribution.\n",
    "\n",
    "To compute the second term, we note that the Beta distribution is a conjugate prior for the Bernoulli distribution. Therefore, if the likelihood function p(x|z) is Bernoulli-distributed, then the posterior distribution p(z|x) is also Beta-distributed. The updated parameters of the posterior distribution can be computed as follows:\n",
    "\n",
    "α' = α + ∑ᵢ₌₁ᴺ xᵢ\n",
    "\n",
    "β' = β + ∑ᵢ₌₁ᴺ (1 - xᵢ)\n",
    "\n",
    "where N is the number of observed data points and x_i is the ith data point.\n",
    "\n",
    "We can then compute the second term as follows:\n",
    "\n",
    "∫ q(z|x) log(p(z)) dz = log B(α', β') - log B(α, β) + (α - α') ψ(α') + (β - β') ψ(β') + (α' + β' - α - β) ψ(α' + β')\n",
    "\n",
    "where B is the Beta function and ψ is the digamma function.\n",
    "\n",
    "Finally, the KLD can be computed as:\n",
    "\n",
    "D_KL(q(z|x) || p(z)) = -H(q(z|x)) + log(B(α', β') / B(α, β)) + (α - α') ψ(α') + (β - β') ψ(β') + (α' + β' - α - β) ψ(α' + β')\n",
    "\n",
    "For a more detailed derivation of the KLD in VAE models with Beta-distributed latent variables, you can refer to the following paper:\n",
    "\n",
    "\"β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\" by Iulia-Alexandra Ganea and Thomas Kober. (https://openreview.net/forum?id=Sy2fzU9gl)\n",
    "\n",
    "This paper introduces the β-VAE framework, which is a modification of the standard VAE that includes a tunable hyperparameter β to control the trade-off between reconstruction accuracy and latent space regularization. The paper derives the KLD formula for the case where the latent variable is assumed to be Beta-distributed, and also provides empirical results demonstrating the effectiveness of the β-VAE framework on a variety of image datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def kl_divergence_beta(p_alpha, p_beta, q_alpha, q_beta):\n",
    "    def log_beta(z, alpha, beta):\n",
    "        return (alpha - 1) * tf.math.log(z) + (beta - 1) * tf.math.log(1 - z) - beta_func(alpha, beta)\n",
    "\n",
    "    def entropy_beta(alpha, beta):\n",
    "        return beta_func(alpha, beta) - (alpha - 1) * tf.math.digamma(alpha) - (beta - 1) * tf.math.digamma(beta) + (alpha + beta - 2) * tf.math.digamma(alpha + beta)\n",
    "\n",
    "    def beta_func(alpha, beta):\n",
    "        return tf.math.lgamma(alpha) + tf.math.lgamma(beta) - tf.math.lgamma(alpha + beta)\n",
    "\n",
    "    kld = entropy_beta(q_alpha, q_beta) - entropy_beta(p_alpha, p_beta)\n",
    "    kld += log_beta(1e-6, p_alpha, p_beta) - log_beta(1e-6, q_alpha, q_beta)\n",
    "    kld += (p_alpha - q_alpha) * tf.math.digamma(q_alpha) + (p_beta - q_beta) * tf.math.digamma(q_beta)\n",
    "    kld += (q_alpha - p_alpha + q_beta - p_beta) * tf.math.digamma(q_alpha + q_beta)\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence_beta(1.,2., 1., 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this code assumes that p_alpha and p_beta are the prior parameters of the beta distribution on the latent variable, and q_alpha and q_beta are the approximate posterior parameters learned by the VAE. \n",
    "\n",
    "You may need to modify the code to match the specific implementation of your VAE.\n",
    "\n",
    "For a more complete example of a VAE with beta-distributed latent variables in TensorFlow,\n",
    "\n",
    "you may refer to the following GitHub repository: https://github.com/Schlumberger/joint-vae/blob/master/jointvae/models.py\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=13.618285>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
