{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Masking, Reshape, Flatten, RepeatVector, TimeDistributed, Bidirectional, Activation, GaussianNoise, Lambda, LSTM\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from Models.FeatExtModels_NoKaiser import *\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.load('./Data/AsanTRSet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Env setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Results/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "### Model checkpoint\n",
    "ModelSaveSameName = save_path+'SigBandRepModel_ExtRec_check.hdf5'\n",
    "ModelSave = ModelCheckpoint(filepath=ModelSaveSameName, monitor='val_mse', verbose=1, save_best_only=True )\n",
    "\n",
    "### Model Early stop\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', patience=500)\n",
    "\n",
    "LatDim = 3\n",
    "SigDim = DATA.shape[1]\n",
    "MaskingRate = 0.02\n",
    "NoiseStd = 0.002\n",
    "MaskStd = 0.1\n",
    "ReparaStd = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, LossName1, LossName2, BetaName1, BetaName2, verbose=1):\n",
    "                \n",
    "        self.LossName1 = LossName1\n",
    "        self.LossName2 = LossName2\n",
    "        self.BetaName1 = BetaName1\n",
    "        self.BetaName2 = BetaName2\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Loss1 = logs[self.LossName1] \n",
    "        Loss2 = logs[self.LossName2] \n",
    "        \n",
    "        Beta1_idx = [num for num, i in enumerate(self.model.variables) if self.BetaName1 in i.name][0]\n",
    "        Beta2_idx = [num for num, i in enumerate(self.model.variables) if self.BetaName2 in i.name][0]\n",
    "        \n",
    "        self.model.variables[Beta1_idx].assign(Loss1/Loss2)\n",
    "        self.model.variables[Beta2_idx].assign(Loss2/Loss1)   \n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName1+' : ', self.model.variables[Beta1_idx])\n",
    "            print(self.BetaName2+' : ', self.model.variables[Beta2_idx])        \n",
    " \n",
    "\n",
    "# Define the KL annealing callback function\n",
    "class KLCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold, MaxBeta, BetaName, AnnealEpoch=100):\n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.BetaName = BetaName\n",
    "        self.MaxBeta = MaxBeta\n",
    "        self.AnnealStart = 0\n",
    "        self.AnnealEpoch = AnnealEpoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = logs['val_'+self.TargetLossName]\n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            self.AnnealStart = 0\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], 0.)\n",
    "        else: \n",
    "            self.AnnealStart += 1\n",
    "            Beta = (self.AnnealStart) / self.AnnealEpoch * self.MaxBeta\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], Beta)\n",
    "        \n",
    "        print(self.model.get_layer(self.BetaName).variables[0])\n",
    "'''\n",
    "\n",
    "'''\n",
    "class KLAnneal(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold,  BetaName, MaxBeta=0.1, MinBeta=1e-5, AnnealEpoch=100, UnderLimit=0., verbose=1):\n",
    "        \n",
    "        if type(TargetLossName) != list:\n",
    "            TargetLossName = [TargetLossName]\n",
    "        \n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.BetaName = BetaName\n",
    "        self.AnnealIdx = 0\n",
    "        self.verbose = verbose \n",
    "        self.Beta =  np.concatenate([np.array([UnderLimit]), np.linspace(start=MinBeta, stop=MaxBeta, num=AnnealEpoch )])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = max([logs[i] for i in self.TargetLossName]) \n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            #self.AnnealIdx -= 1\n",
    "            #self.AnnealIdx = np.maximum(self.AnnealIdx, 0)\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], self.Beta[self.AnnealIdx])\n",
    "        else: \n",
    "            self.AnnealIdx += 1\n",
    "            self.AnnealIdx = np.minimum(self.AnnealIdx, len(self.Beta)-1)\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], self.Beta[self.AnnealIdx])\n",
    "        \n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "        elif self.verbose==2:\n",
    "            print('TargetLoss : ', TargetLoss)\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "'''\n",
    "\n",
    "'''\n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, LossName1, LossName2, Beta1, Beta2, WeightB1 =1., WeightB2 =1., BetaName1 ='Beta1' , BetaName2 ='Beta2' , verbose=1):\n",
    "                \n",
    "        self.LossName1 = LossName1\n",
    "        self.LossName2 = LossName2\n",
    "        self.Beta1 = Beta1\n",
    "        self.Beta2 = Beta2\n",
    "        self.BetaName1 = BetaName1\n",
    "        self.BetaName2 = BetaName2\n",
    "        self.verbose = verbose\n",
    "        self.WeightB1 = WeightB1\n",
    "        self.WeightB2 = WeightB2\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Loss1 = logs[self.LossName1] \n",
    "        Loss2 = logs[self.LossName2] \n",
    "        \n",
    "        Beta1 = tf.maximum(Loss1/Loss2, 1.) * self.WeightB1\n",
    "        Beta2 = tf.maximum(Loss2/Loss1, 1.) * self.WeightB2\n",
    "        \n",
    "        self.Beta1.assign(Beta1)\n",
    "        self.Beta2.assign(Beta2)   \n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName1+' : ', self.Beta1.numpy())\n",
    "            print(self.BetaName2+' : ', self.Beta2.numpy())   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class KLAnneal(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold,  Beta, BetaName ='Beta', MaxBeta=0.1, MinBeta=1e-5, AnnealEpoch=100, UnderLimit=0., verbose=1):\n",
    "        \n",
    "        if type(TargetLossName) != list:\n",
    "            TargetLossName = [TargetLossName]\n",
    "        \n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.Beta = Beta\n",
    "        self.BetaName = BetaName\n",
    "        self.AnnealIdx = 0\n",
    "        self.verbose = verbose \n",
    "        self.BetaValue =  np.concatenate([np.array([UnderLimit]), np.linspace(start=MinBeta, stop=MaxBeta, num=AnnealEpoch )])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = max([logs[i] for i in self.TargetLossName]) \n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            #self.AnnealIdx -= 1\n",
    "            #self.AnnealIdx = np.maximum(self.AnnealIdx, 0)\n",
    "            self.Beta.assign(self.BetaValue[self.AnnealIdx])\n",
    "        else: \n",
    "            self.AnnealIdx += 1\n",
    "            self.AnnealIdx = np.minimum(self.AnnealIdx, len(self.BetaValue)-1)\n",
    "            self.Beta.assign(self.BetaValue[self.AnnealIdx])\n",
    "        \n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName+' : ' ,self.Beta.numpy())\n",
    "        elif self.verbose==2:\n",
    "            print('TargetLoss : ', TargetLoss)\n",
    "            print(self.BetaName+' : ' ,self.Beta.numpy())\n",
    "            \n",
    "            \n",
    "       \n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, BetaList, BetaWeight, MinLimit , MaxLimit , verbose=1):\n",
    "                \n",
    "        self.BetaList = BetaList\n",
    "        self.BetaWeight = BetaWeight\n",
    "        self.MinLimit = MinLimit\n",
    "        self.MaxLimit = MaxLimit\n",
    "        self.verbose = verbose\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Losses = np.array([logs[i] for i in self.BetaList.keys()])\n",
    "        Losses = np.maximum(1e-7, Losses)\n",
    "        RelWeights = Losses / np.min(Losses)\n",
    "        RelWeights = {LossName:RelWeights[num] for num, LossName in enumerate (self.BetaList.keys())}\n",
    "\n",
    "        for name, beta in self.BetaList.items():\n",
    "            \n",
    "            Value = np.clip(RelWeights[name] * self.BetaWeight[name], self.MinLimit[name], self.MaxLimit[name])\n",
    "            beta.assign(Value)\n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(RelWeights)\n",
    "            for key, beta in self.BetaList.items():\n",
    "                print(key, ': ', beta.numpy())\n",
    "                \n",
    "RelLossDic = {'val_mse':'Beta_Rec', 'val_FeatRecLoss':'Beta_Feat', 'val_kl_Loss_Z':'Beta_Z', 'val_kl_Loss_FC':'Beta_Fc'}\n",
    "WeightDic = {'val_mse':10., 'val_FeatRecLoss':100., 'val_kl_Loss_Z':1., 'val_kl_Loss_FC':1.}\n",
    "MinLimit = {'val_mse':1., 'val_FeatRecLoss':1., 'val_kl_Loss_Z':0.01, 'val_kl_Loss_FC':0.01}\n",
    "MaxLimit = {'val_mse':200., 'val_FeatRecLoss':200., 'val_kl_Loss_Z':0.1, 'val_kl_Loss_FC':0.1}\n",
    "RelLoss = RelLossWeight(BetaList=RelLossDic, BetaWeight= WeightDic, MinLimit= MinLimit, MaxLimit = MaxLimit )\n",
    "           \n",
    "'''        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def ReName (layer, name):\n",
    "    return Lambda(lambda x: x, name=name)(layer)\n",
    "\n",
    "def ParaFilters (layer, name=''):\n",
    "    Fc = Dense(1, activation='sigmoid')(layer)\n",
    "    Fc = tf.clip_by_value(Fc, 1e-7, 1-1e-7)\n",
    "    \n",
    "    # Reparameterization Trick for sampling from Uniformly distribution; ϵ∼U(0,1) \n",
    "    Epsilon = tf.random.uniform(shape=(tf.shape(Fc)[0], Fc.shape[1]))\n",
    "    Epsilon = tf.clip_by_value(Epsilon, 1e-7, 1-1e-7)\n",
    "\n",
    "    LogEps = tf.math.log(Epsilon)\n",
    "    LogNegEps = tf.math.log(1 - Epsilon)\n",
    "    \n",
    "    LogTheta = tf.math.log(Fc)\n",
    "    LogNegTheta = tf.math.log(1-Fc)\n",
    "\n",
    "    Fc = tf.math.sigmoid(LogEps - LogNegEps + LogTheta - LogNegTheta)\n",
    "    Fc = tf.clip_by_value(Fc, 1e-7, 1-1e-7)\n",
    "    Fc = ReName(Fc, name)\n",
    "    \n",
    "    return Fc \n",
    "\n",
    "\n",
    "class Lossweight(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, InitVal = 0., name='Lossweight'):\n",
    "        super(Lossweight, self).__init__(name=name)\n",
    "        self.InitVal = InitVal\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.GenVec = tf.Variable(self.InitVal, trainable=False)\n",
    "    \n",
    "    def call(self, input):\n",
    "\n",
    "        return self.GenVec\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Lossweight, self).get_config()\n",
    "        config.update({ 'InitVal': self.InitVal })\n",
    "        return config\n",
    "    \n",
    "\n",
    "class RandFCs(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super(RandFCs, self).__init__(name='FCs')\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.GenVec = tf.Variable(tf.random.uniform(shape=(1,6)), trainable=False)\n",
    "    \n",
    "    def call(self, input):\n",
    "        return tf.tile(self.GenVec , (tf.shape(input)[0],1))\n",
    "    \n",
    "\n",
    "\n",
    "class KLAnneal(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold,  BetaName, MaxBeta=0.1, MinBeta=1e-5, AnnealEpoch=100, UnderLimit=0., verbose=1):\n",
    "        \n",
    "        if type(TargetLossName) != list:\n",
    "            TargetLossName = [TargetLossName]\n",
    "        \n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.BetaName = BetaName\n",
    "        self.AnnealIdx = 0\n",
    "        self.verbose = verbose \n",
    "        self.Beta =  np.concatenate([np.array([UnderLimit]), np.linspace(start=MinBeta, stop=MaxBeta, num=AnnealEpoch )])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = max([logs[i] for i in self.TargetLossName]) \n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            self.AnnealIdx -= 1\n",
    "            self.AnnealIdx = np.maximum(self.AnnealIdx, 0)\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], self.Beta[self.AnnealIdx])\n",
    "        else: \n",
    "            self.AnnealIdx += 1\n",
    "            self.AnnealIdx = np.minimum(self.AnnealIdx, len(self.Beta)-1)\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], self.Beta[self.AnnealIdx])\n",
    "        \n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "        elif self.verbose==2:\n",
    "            print('TargetLoss : ', TargetLoss)\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "            \n",
    "          \n",
    "\n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, BetaList, LossScaling, MinLimit , MaxLimit , verbose=1):\n",
    "                \n",
    "        self.BetaList = BetaList\n",
    "        self.LossScaling = LossScaling\n",
    "        self.MinLimit = MinLimit\n",
    "        self.MaxLimit = MaxLimit\n",
    "        self.verbose = verbose\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Losses = np.array([logs[i] for i in self.BetaList.keys()])\n",
    "        Losses = np.maximum(1e-7, Losses)\n",
    "        RelWeights = Losses / np.min(Losses)\n",
    "        RelWeights = {loss:RelWeights[num] * self.LossScaling[loss] for num, loss in enumerate (self.BetaList.keys())}\n",
    "\n",
    "        for loss, beta in self.BetaList.items():\n",
    "            \n",
    "            Value = np.clip(RelWeights[loss] , self.MinLimit[beta], self.MaxLimit[beta])\n",
    "            K.set_value(self.model.get_layer(beta).variables[0], Value)\n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(RelWeights)\n",
    "            for key, beta in self.BetaList.items():\n",
    "                print(beta, ': ', self.model.get_layer(beta).variables[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(SigDim, LatDim= 2, Type = '', MaskingRate = 0.025, NoiseStd = 0.002, MaskStd = 0.1, ReparaStd = 0.1 , Reparam = False):\n",
    "\n",
    "    InpL = Input(shape=(SigDim,))\n",
    "    InpFrame = tf.signal.frame(InpL, 100, 100)\n",
    "\n",
    "    if Reparam:\n",
    "        InpRegul = GaussianNoise(stddev=NoiseStd)(InpFrame, training=Reparam)\n",
    "        MaskVec, NoisVec = MaskingGen(InpRegul, MaskingRate, MaskStd)\n",
    "        EncInp = Masking(mask_value=0.)(InpRegul * MaskVec )\n",
    "        EncOut = InpRegul + NoisVec\n",
    "    else:\n",
    "        EncInp, EncOut = InpFrame, InpFrame\n",
    "\n",
    "    Encoder = Dense(50, activation='relu')(InpFrame)\n",
    "    Encoder = Bidirectional(GRU(30, return_sequences=True))(Encoder)\n",
    "    Encoder = Bidirectional(GRU(30, return_sequences=False))(Encoder)\n",
    "    Encoder = Dense(50, activation='relu')(Encoder)\n",
    "    Encoder = Dense(30, activation='relu')(Encoder)\n",
    "    Encoder = Dense(15, activation='relu')(Encoder)\n",
    "\n",
    "    Z_Mean = Dense(LatDim, activation='linear')(Encoder)\n",
    "    Z_Log_Sigma = Dense(LatDim, activation='relu')(Encoder)\n",
    "    Z_Log_Sigma = ReName(Z_Log_Sigma,'Z_Log_Sigma_'+Type)\n",
    "\n",
    "    \n",
    "    # Reparameterization Trick for sampling from Guassian distribution\n",
    "    Epsilon = tf.random.normal(shape=(tf.shape(Z_Mean)[0], Z_Mean.shape[1]), mean=0., stddev=ReparaStd)\n",
    "\n",
    "    if Reparam==False:\n",
    "        Epsilon = Epsilon * 0\n",
    "\n",
    "    Z_Mean = Z_Mean + tf.exp(0.5 * Z_Log_Sigma) * Epsilon\n",
    "    Z_Mean = ReName(Z_Mean,'Z_Mean_'+Type)\n",
    "    \n",
    "    FCs =   Dense(6, activation='relu')(Encoder)\n",
    "    FCs =   Dense(6, activation='sigmoid')(FCs)\n",
    "    \n",
    "    \n",
    "    # Reparameterization Trick for sampling from Uniformly distribution; ϵ∼U(0,1) \n",
    "    FCs = tf.clip_by_value(FCs, 1e-7, 1-1e-7)\n",
    "    Epsilon = tf.random.uniform(shape=(tf.shape(FCs)[0], FCs.shape[1]))\n",
    "    Epsilon = tf.clip_by_value(Epsilon, 1e-7, 1-1e-7)\n",
    "    \n",
    "    LogEps = tf.math.log(Epsilon)\n",
    "    LogNegEps = tf.math.log(1 - Epsilon)\n",
    "    \n",
    "    LogTheta = tf.math.log(FCs)\n",
    "    LogNegTheta = tf.math.log(1-FCs)\n",
    "    \n",
    "    \n",
    "    FCs = tf.math.sigmoid(LogEps - LogNegEps + LogTheta - LogNegTheta)\n",
    "    FCs = tf.clip_by_value(FCs, 1e-7, 1-1e-7)\n",
    "    FCs = ReName(FCs, 'FCs')\n",
    "    \n",
    "    return [InpL], [Flatten()(EncOut), Z_Mean, FCs]\n",
    "\n",
    "\n",
    "\n",
    "def FeatExtractor(Inps, LatDim= 2, FiltLenList = [301, 301, 301, 301, 301, 301] ):\n",
    "    \n",
    "    EncReInp, InpZ, FCs = Inps\n",
    "    \n",
    "    H_F, L_F, HH_F, HL_F, LH_F, LL_F = tf.split(FCs, 6, axis=1)\n",
    "    \n",
    "\n",
    "    ### Filtering level 1 -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_H = GenHighFilter(H_F,  N=FiltLenList[0])\n",
    "    To_L = GenLowFilter(L_F, N=FiltLenList[1])\n",
    "\n",
    "    ## Perform signal filtering level 1\n",
    "    InpFrame =  tf.signal.frame(EncReInp, To_H.shape[-1], 1)\n",
    "    Sig_H = tf.reduce_sum(InpFrame*To_H[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_H = ReName(Sig_H, 'Sig_H_Ext')\n",
    "\n",
    "    InpFrame =  tf.signal.frame(EncReInp, To_L.shape[-1], 1)\n",
    "    Sig_L = tf.reduce_sum(InpFrame*To_L[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_L = ReName(Sig_L, 'Sig_L_Ext')\n",
    "\n",
    "\n",
    "\n",
    "    ### Filtering level HH and HL (from Sig_H) -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_HH = GenHighFilter(HH_F, N=FiltLenList[2])\n",
    "    To_HL = GenLowFilter(HL_F, N=FiltLenList[3])\n",
    "\n",
    "    ## Perform signal filtering level 2\n",
    "    Frame_H =  tf.signal.frame(Sig_H[:,:,0], To_HH.shape[-1], 1)\n",
    "    Sig_HH = tf.reduce_sum(Frame_H*To_HH[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_HH = ReName(Sig_HH, 'Sig_HH_Ext')\n",
    "\n",
    "    Frame_H =  tf.signal.frame(Sig_H[:,:,0], To_HL.shape[-1], 1)\n",
    "    Sig_HL = tf.reduce_sum(Frame_H*To_HL[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_HL = ReName(Sig_HL, 'Sig_HL_Ext')\n",
    "\n",
    "\n",
    "    ### Filtering level LH and LL (from Sig_L) -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_LH = GenHighFilter(LH_F,  N=FiltLenList[4])\n",
    "    To_LL = GenLowFilter(LL_F,  N=FiltLenList[5])\n",
    "\n",
    "    ## Perform signal filtering level 2\n",
    "    Frame_L =  tf.signal.frame(Sig_L[:,:,0], To_LH.shape[-1], 1)\n",
    "    Sig_LH = tf.reduce_sum(Frame_L*To_LH[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_LH = ReName(Sig_LH, 'Sig_LH_Ext')\n",
    "\n",
    "    Frame_L =  tf.signal.frame(Sig_L[:,:,0], To_LL.shape[-1], 1)\n",
    "    Sig_LL = tf.reduce_sum(Frame_L*To_LL[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_LL = ReName(Sig_LL, 'Sig_LL_Ext')\n",
    "\n",
    "    \n",
    "    return [Flatten()(Sig_HH), Flatten()(Sig_HL), Flatten()(Sig_LH), Flatten()(Sig_LL)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FeatGenerator (Inp_lat):\n",
    "    \n",
    "    #Inp_lat = tf.concat(Inp_lat, axis=-1)\n",
    "    InpZ, FCCommon, FCEach = Inp_lat\n",
    "    HH_F, HL_F, LH_F, LL_F = tf.split(FCEach, 4, axis=1)\n",
    "    \n",
    "   \n",
    "    Dec_Sig_HH = Dense(10, activation='relu')(tf.concat([InpZ, FCCommon, HH_F], axis=-1))\n",
    "    Dec_Sig_HH = Dense(20, activation='relu')(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(30, activation='relu')(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(50, activation='relu')(Dec_Sig_HH)\n",
    "\n",
    "    Dec_Sig_HH = RepeatVector(10 )(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(40,'tanh')(Dec_Sig_HH)\n",
    "    Sig_HH= Flatten(name='Sig_HH_Gen')(Dec_Sig_HH)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_HL = Dense(10, activation='relu')(tf.concat([InpZ, FCCommon, HL_F], axis=-1))\n",
    "    Dec_Sig_HL = Dense(20, activation='relu')(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(30, activation='relu')(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(50, activation='relu')(Dec_Sig_HL)\n",
    "\n",
    "    Dec_Sig_HL = RepeatVector(10 )(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(40,'tanh')(Dec_Sig_HL)\n",
    "    Sig_HL= Flatten(name='Sig_HL_Gen')(Dec_Sig_HL)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_LH = Dense(10, activation='relu')(tf.concat([InpZ, FCCommon, LH_F], axis=-1))\n",
    "    Dec_Sig_LH = Dense(20, activation='relu')(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(30, activation='relu')(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(50, activation='relu')(Dec_Sig_LH)\n",
    "\n",
    "    Dec_Sig_LH = RepeatVector(10 )(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(40,'tanh')(Dec_Sig_LH)\n",
    "    Sig_LH= Flatten(name='Sig_LH_Gen')(Dec_Sig_LH)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_LL = Dense(10, activation='relu')(tf.concat([InpZ, FCCommon, LL_F], axis=-1))\n",
    "    Dec_Sig_LL = Dense(20, activation='relu')(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(30, activation='relu')(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(50, activation='relu')(Dec_Sig_LL)\n",
    "\n",
    "    Dec_Sig_LL = RepeatVector(10 )(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(40,'tanh')(Dec_Sig_LL)\n",
    "    Sig_LL= Flatten(name='Sig_LL_Gen')(Dec_Sig_LL)\n",
    "    \n",
    "    return  Sig_HH, Sig_HL, Sig_LH, Sig_LL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Reconstructor(Inps ):\n",
    "    Sig_HH, Sig_HL, Sig_LH, Sig_LL = Inps\n",
    "\n",
    "    ## GRU NET -------------------------------------------------------------------\n",
    "    Dec_Sig_HH = Reshape((-1, 100))(Sig_HH)\n",
    "    Dec_Sig_HL = Reshape((-1, 100))(Sig_HL)\n",
    "    Dec_Sig_LH = Reshape((-1, 100))(Sig_LH)\n",
    "    Dec_Sig_LL = Reshape((-1, 100))(Sig_LL)\n",
    "\n",
    "    Dec_Sig_HH = Bidirectional(GRU(5), name='Dec_Sig_HH')(Dec_Sig_HH)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(5), name='Dec_Sig_HL')(Dec_Sig_HL)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(5), name='Dec_Sig_LH')(Dec_Sig_LH)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(5), name='Dec_Sig_LL')(Dec_Sig_LL)\n",
    "\n",
    "    Decoder = tf.concat([ Dec_Sig_HH, Dec_Sig_HL, Dec_Sig_LH, Dec_Sig_LL], axis=1)\n",
    "    Decoder = RepeatVector((SigDim//100) )(Decoder)\n",
    "    Decoder = Bidirectional(GRU(50, return_sequences=True))(Decoder)\n",
    "    Decoder = Dense(100, activation='relu')(Decoder)\n",
    "    DecOut = Dense(100, activation='sigmoid')(Decoder)\n",
    "    DecOut = Reshape((SigDim,),name='Out')(DecOut)\n",
    "\n",
    "    \n",
    "    return DecOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder - FeatExtractor\n",
    "EncInp, EncOut = Encoder(SigDim=SigDim, LatDim= LatDim, Type = 'Tr', MaskingRate = MaskingRate, NoiseStd = NoiseStd, MaskStd = MaskStd, ReparaStd = ReparaStd, Reparam=True)\n",
    "FeatExtOut = FeatExtractor(EncOut)\n",
    "\n",
    "### Encoder - FeatGenerator - Reconstruction\n",
    "FeatGenOut = FeatGenerator([EncOut[1],EncOut[2][:, :2], EncOut[2][:, 2:]])\n",
    "FeatGenOut = ReName(FeatGenOut, 'FeatGenOut')\n",
    "\n",
    "ReconOut = Reconstructor(FeatExtOut)\n",
    "ReconOut = ReName(ReconOut, 'ReconOut')\n",
    "\n",
    "### Define the total model\n",
    "SigBandRepModel = Model(EncInp, ReconOut)\n",
    "\n",
    "### Weight controller; Apply beta and capacity \n",
    "Capacity_Z = 0.1 # 0.1 0.05\n",
    "Capacity_Fc = 0.6\n",
    "Beta_Z = Lossweight(name='Beta_Z')(FeatGenOut)\n",
    "Beta_Fc = Lossweight(name='Beta_Fc')(FeatGenOut)\n",
    "Beta_Rec = Lossweight(name='Beta_Rec')(FeatGenOut)\n",
    "Beta_Feat = Lossweight(name='Beta_Feat')(FeatGenOut)\n",
    "\n",
    "\n",
    "### Adding the RecLoss; \n",
    "MSE = tf.keras.losses.MeanSquaredError()\n",
    "RecLoss = MSE(ReconOut, EncInp)\n",
    "SigBandRepModel.add_loss(RecLoss * Beta_Rec )\n",
    "SigBandRepModel.add_metric(RecLoss, 'RecLoss')\n",
    "\n",
    "\n",
    "### Adding the FeatRecLoss; It allows connection between the extractor and generator\n",
    "FeatRecLoss= MSE(tf.concat(FeatGenOut, axis=-1), tf.concat(FeatExtOut, axis=-1))\n",
    "SigBandRepModel.add_loss(FeatRecLoss * Beta_Feat )\n",
    "SigBandRepModel.add_metric(FeatRecLoss, 'FeatRecLoss')\n",
    "\n",
    "### KL Divergence for p(Z) vs q(Z)\n",
    "Z_Sampled, Z_Log_Sigma = SigBandRepModel.get_layer('Z_Mean_Tr').output, SigBandRepModel.get_layer('Z_Log_Sigma_Tr').output\n",
    "kl_Loss_Z = 0.5 * tf.reduce_sum( Z_Sampled**2  +  tf.exp(Z_Log_Sigma)- Z_Log_Sigma-1, axis=1)    \n",
    "kl_Loss_Z = tf.reduce_mean(kl_Loss_Z )\n",
    "kl_Loss_Z =  tf.abs(kl_Loss_Z - Capacity_Z)\n",
    "\n",
    "### KL Divergence for p(FCs) vs q(FCs)\n",
    "BernP = 0.5 # hyperparameter\n",
    "FCs = SigBandRepModel.get_layer('FCs').output\n",
    "kl_Loss_FC = tf.math.log(FCs) - tf.math.log(BernP) + tf.math.log(1-FCs) - tf.math.log(1-BernP) \n",
    "kl_Loss_FC = tf.reduce_mean(-kl_Loss_FC )\n",
    "kl_Loss_FC = tf.abs(kl_Loss_FC - Capacity_Fc)\n",
    "\n",
    "SigBandRepModel.add_loss(kl_Loss_Z * Beta_Z)\n",
    "SigBandRepModel.add_metric(kl_Loss_Z, 'kl_Loss_Z')\n",
    "\n",
    "SigBandRepModel.add_loss(kl_Loss_FC * Beta_Fc )\n",
    "SigBandRepModel.add_metric(kl_Loss_FC, 'kl_Loss_FC')\n",
    "\n",
    "## Model Compile\n",
    "SigBandRepModel.compile(optimizer='adam') \n",
    "\n",
    "### Loss and KLD_Beta controller\n",
    "KLD_Beta_Z = KLAnneal(TargetLossName=['val_FeatRecLoss', 'val_RecLoss'], Threshold=0.001, BetaName='Beta_Z',  MaxBeta=0.1 , MinBeta=0.1, AnnealEpoch=1, UnderLimit=1e-7, verbose=2)\n",
    "KLD_Beta_Fc = KLAnneal(TargetLossName=['val_FeatRecLoss', 'val_RecLoss'], Threshold=0.001, BetaName='Beta_Fc',  MaxBeta=0.005 , MinBeta=0.005, AnnealEpoch=1, UnderLimit=1e-7, verbose=1)\n",
    "\n",
    "RelLossDic = {'val_RecLoss':'Beta_Rec', 'val_FeatRecLoss':'Beta_Feat', 'val_kl_Loss_Z':'Beta_Z', 'val_kl_Loss_FC':'Beta_Fc'}\n",
    "ScalingDic = {'val_RecLoss':100., 'val_FeatRecLoss':100., 'val_kl_Loss_Z':0.1, 'val_kl_Loss_FC':0.1}\n",
    "MinLimit = {'Beta_Rec':1., 'Beta_Feat':1., 'Beta_Z':0.01, 'Beta_Fc':0.01}\n",
    "MaxLimit = {'Beta_Rec':500., 'Beta_Feat':500., 'Beta_Z':0.1, 'Beta_Fc':0.1}\n",
    "RelLoss = RelLossWeight(BetaList=RelLossDic, LossScaling= ScalingDic, MinLimit= MinLimit, MaxLimit = MaxLimit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      " 6/92 [>.............................] - ETA: 40s - loss: 0.4923 - RecLoss: 0.0030 - FeatRecLoss: 0.0017 - kl_Loss_Z: 0.2557 - kl_Loss_FC: 0.2992  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1378s vs `on_train_batch_end` time: 0.2811s). Check your callbacks.\n",
      "92/92 [==============================] - 69s 547ms/step - loss: 0.3377 - RecLoss: 0.0012 - FeatRecLoss: 0.0010 - kl_Loss_Z: 0.2433 - kl_Loss_FC: 0.2860 - val_loss: 0.2376 - val_RecLoss: 9.9853e-04 - val_FeatRecLoss: 9.3011e-04 - val_kl_Loss_Z: 0.2334 - val_kl_Loss_FC: 0.2874\n",
      "\n",
      "Epoch 00001: val_FeatRecLoss improved from inf to 0.00093, saving model to ./Results\\SigBandRepModel_ExtRec_check.hdf5\n",
      "{'val_RecLoss': 107.35546595878178, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 25.088328297843344, 'val_kl_Loss_FC': 30.903048316547604}\n",
      "Beta_Rec :  107.35547\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.1\n",
      "Epoch 2/700\n",
      "92/92 [==============================] - 46s 498ms/step - loss: 0.2506 - RecLoss: 0.0010 - FeatRecLoss: 9.8841e-04 - kl_Loss_Z: 0.2301 - kl_Loss_FC: 0.2044 - val_loss: 0.2503 - val_RecLoss: 0.0010 - val_FeatRecLoss: 0.0010 - val_kl_Loss_Z: 0.2271 - val_kl_Loss_FC: 0.1920\n",
      "\n",
      "Epoch 00002: val_FeatRecLoss did not improve from 0.00093\n",
      "{'val_RecLoss': 100.16136754911224, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 22.617461176838848, 'val_kl_Loss_FC': 19.12018744094648}\n",
      "Beta_Rec :  100.16137\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.1\n",
      "Epoch 3/700\n",
      "92/92 [==============================] - 46s 498ms/step - loss: 0.2419 - RecLoss: 0.0010 - FeatRecLoss: 9.9163e-04 - kl_Loss_Z: 0.2306 - kl_Loss_FC: 0.1914 - val_loss: 0.2431 - val_RecLoss: 0.0010 - val_FeatRecLoss: 0.0010 - val_kl_Loss_Z: 0.2311 - val_kl_Loss_FC: 0.1831\n",
      "\n",
      "Epoch 00003: val_FeatRecLoss did not improve from 0.00093\n",
      "{'val_RecLoss': 100.0, 'val_FeatRecLoss': 100.06607424868285, 'val_kl_Loss_Z': 22.942636316284347, 'val_kl_Loss_FC': 18.177961856801982}\n",
      "Beta_Rec :  100.0\n",
      "Beta_Feat :  100.06607\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.1\n",
      "Epoch 4/700\n",
      "92/92 [==============================] - 46s 499ms/step - loss: 0.2417 - RecLoss: 0.0010 - FeatRecLoss: 9.8966e-04 - kl_Loss_Z: 0.2319 - kl_Loss_FC: 0.1898 - val_loss: 0.2422 - val_RecLoss: 0.0010 - val_FeatRecLoss: 9.8492e-04 - val_kl_Loss_Z: 0.2348 - val_kl_Loss_FC: 0.1952\n",
      "\n",
      "Epoch 00004: val_FeatRecLoss did not improve from 0.00093\n",
      "{'val_RecLoss': 102.145545730001, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 23.844008054026038, 'val_kl_Loss_FC': 19.821187005525175}\n",
      "Beta_Rec :  102.145546\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.1\n",
      "Epoch 5/700\n",
      "92/92 [==============================] - 46s 498ms/step - loss: 0.2439 - RecLoss: 0.0010 - FeatRecLoss: 9.9498e-04 - kl_Loss_Z: 0.2335 - kl_Loss_FC: 0.1895 - val_loss: 0.2444 - val_RecLoss: 0.0010 - val_FeatRecLoss: 0.0010 - val_kl_Loss_Z: 0.2335 - val_kl_Loss_FC: 0.1832\n",
      "\n",
      "Epoch 00005: val_FeatRecLoss did not improve from 0.00093\n",
      "{'val_RecLoss': 100.45752918883142, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 23.3276088132415, 'val_kl_Loss_FC': 18.30852152185571}\n",
      "Beta_Rec :  100.45753\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.1\n",
      "Epoch 6/700\n",
      "41/92 [============>.................] - ETA: 24s - loss: 0.2422 - RecLoss: 0.0010 - FeatRecLoss: 9.9721e-04 - kl_Loss_Z: 0.2324 - kl_Loss_FC: 0.1872"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m SigBandRepModel\u001b[38;5;241m.\u001b[39mload_weights(ModelSaveSameName)\n\u001b[0;32m      2\u001b[0m ModelSave \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mModelSaveSameName, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_FeatRecLoss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mSigBandRepModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m700\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelSave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRelLoss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[1;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2940\u001b[0m   (graph_function,\n\u001b[0;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m     args,\n\u001b[0;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1923\u001b[0m     executing_eagerly)\n\u001b[0;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SigBandRepModel.load_weights(ModelSaveSameName)\n",
    "ModelSave = ModelCheckpoint(filepath=ModelSaveSameName, monitor='val_FeatRecLoss', verbose=1, save_best_only=True)\n",
    "\n",
    "SigBandRepModel.fit(DATA[:], batch_size=3500, epochs=700, shuffle=True, validation_split=0.2, callbacks=[EarlyStop, ModelSave, RelLoss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range (5000):\n",
    "\n",
    "    FCs = 0.9\n",
    "    FCs = tf.clip_by_value(FCs, 1e-7, 1-1e-7)\n",
    "    Epsilon = tf.random.uniform(shape=(1, 1))\n",
    "    Epsilon = tf.clip_by_value(Epsilon, 1e-7, 1-1e-7)\n",
    "\n",
    "    LogEps = tf.math.log(Epsilon)\n",
    "    LogNegEps = tf.math.log(1 - Epsilon)\n",
    "\n",
    "    LogTheta = tf.math.log(FCs)\n",
    "    LogNegTheta = tf.math.log(1-FCs)\n",
    "\n",
    "\n",
    "    res.append(tf.math.sigmoid(LogEps - LogNegEps + LogTheta - LogNegTheta).numpy())\n",
    "    \n",
    "plt.hist(np.concatenate(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
