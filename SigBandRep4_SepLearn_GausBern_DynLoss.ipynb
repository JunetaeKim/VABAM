{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Masking, Reshape, Flatten, RepeatVector, TimeDistributed, Bidirectional, Activation, GaussianNoise, Lambda, LSTM\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from Models.FeatExtModels_NoKaiser import *\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.load('./Data/AsanTRSet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Env setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Results/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "### Model checkpoint\n",
    "ModelSaveSameName = save_path+'SigBandRepModel_DyLoss.hdf5'\n",
    "ModelSave = ModelCheckpoint(filepath=ModelSaveSameName, monitor='val_mse', verbose=1, save_best_only=True )\n",
    "\n",
    "### Model Early stop\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', patience=500)\n",
    "\n",
    "LatDim = 3\n",
    "SigDim = DATA.shape[1]\n",
    "MaskingRate = 0.02\n",
    "NoiseStd = 0.002\n",
    "MaskStd = 0.1\n",
    "ReparaStd = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, LossName1, LossName2, BetaName1, BetaName2, verbose=1):\n",
    "                \n",
    "        self.LossName1 = LossName1\n",
    "        self.LossName2 = LossName2\n",
    "        self.BetaName1 = BetaName1\n",
    "        self.BetaName2 = BetaName2\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Loss1 = logs[self.LossName1] \n",
    "        Loss2 = logs[self.LossName2] \n",
    "        \n",
    "        Beta1_idx = [num for num, i in enumerate(self.model.variables) if self.BetaName1 in i.name][0]\n",
    "        Beta2_idx = [num for num, i in enumerate(self.model.variables) if self.BetaName2 in i.name][0]\n",
    "        \n",
    "        self.model.variables[Beta1_idx].assign(Loss1/Loss2)\n",
    "        self.model.variables[Beta2_idx].assign(Loss2/Loss1)   \n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName1+' : ', self.model.variables[Beta1_idx])\n",
    "            print(self.BetaName2+' : ', self.model.variables[Beta2_idx])        \n",
    " \n",
    "\n",
    "# Define the KL annealing callback function\n",
    "class KLCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold, MaxBeta, BetaName, AnnealEpoch=100):\n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.BetaName = BetaName\n",
    "        self.MaxBeta = MaxBeta\n",
    "        self.AnnealStart = 0\n",
    "        self.AnnealEpoch = AnnealEpoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = logs['val_'+self.TargetLossName]\n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            self.AnnealStart = 0\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], 0.)\n",
    "        else: \n",
    "            self.AnnealStart += 1\n",
    "            Beta = (self.AnnealStart) / self.AnnealEpoch * self.MaxBeta\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], Beta)\n",
    "        \n",
    "        print(self.model.get_layer(self.BetaName).variables[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParaFilters (layer, name=''):\n",
    "    Fc = Dense(1, activation='sigmoid')(layer)\n",
    "    Fc = tf.clip_by_value(Fc, 1e-7, 1-1e-7)\n",
    "    \n",
    "    # Reparameterization Trick for sampling from Uniformly distribution; ϵ∼U(0,1) \n",
    "    Epsilon = tf.random.uniform(shape=(tf.shape(Fc)[0], Fc.shape[1]))\n",
    "    Epsilon = tf.clip_by_value(Epsilon, 1e-7, 1-1e-7)\n",
    "\n",
    "    LogEps = tf.math.log(Epsilon)\n",
    "    LogNegEps = tf.math.log(1 - Epsilon)\n",
    "    \n",
    "    LogTheta = tf.math.log(Fc)\n",
    "    LogNegTheta = tf.math.log(1-Fc)\n",
    "\n",
    "    Fc = tf.math.sigmoid(LogEps - LogNegEps + LogTheta - LogNegTheta)\n",
    "    Fc = tf.clip_by_value(Fc, 1e-7, 1-1e-7)\n",
    "    Fc = ReName(Fc, name)\n",
    "    \n",
    "    return Fc \n",
    "\n",
    "\n",
    "class Lossweight(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, name='Lossweight'):\n",
    "        super(Lossweight, self).__init__(name=name)\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.GenVec = tf.Variable(0., trainable=False)\n",
    "    \n",
    "    def call(self, input):\n",
    "\n",
    "        return self.GenVec\n",
    "    \n",
    "\n",
    "class RandFCs(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super(RandFCs, self).__init__(name='FCs')\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.GenVec = tf.Variable(tf.random.uniform(shape=(1,6)), trainable=False)\n",
    "    \n",
    "    def call(self, input):\n",
    "        return tf.tile(self.GenVec , (tf.shape(input)[0],1))\n",
    "    \n",
    "\n",
    "\n",
    "class KLAnneal(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold, MaxBeta, BetaName, AnnealEpoch=100, verbose=1):\n",
    "        \n",
    "        if type(TargetLossName) != list:\n",
    "            TargetLossName = [TargetLossName]\n",
    "        \n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.BetaName = BetaName\n",
    "        self.MaxBeta = MaxBeta\n",
    "        self.AnnealStart = 0\n",
    "        self.AnnealEpoch = AnnealEpoch\n",
    "        self.verbose = verbose \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = max([logs[i] for i in self.TargetLossName]) \n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            self.AnnealStart = 0\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], 0.)\n",
    "        else: \n",
    "            self.AnnealStart += 1\n",
    "            Beta = (self.AnnealStart) / self.AnnealEpoch * self.MaxBeta\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], Beta)\n",
    "        \n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "        elif self.verbose==2:\n",
    "            print('TargetLoss : ', TargetLoss)\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "            \n",
    "            \n",
    "       \n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, LossName1, LossName2, Beta1, Beta2, Weight =1., BetaName1 ='Beta1' , BetaName2 ='Beta2' , verbose=1):\n",
    "                \n",
    "        self.LossName1 = LossName1\n",
    "        self.LossName2 = LossName2\n",
    "        self.Beta1 = Beta1\n",
    "        self.Beta2 = Beta2\n",
    "        self.BetaName1 = BetaName1\n",
    "        self.BetaName2 = BetaName2\n",
    "        self.verbose = verbose\n",
    "        self.Weight = Weight\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Loss1 = logs[self.LossName1] \n",
    "        Loss2 = logs[self.LossName2] \n",
    "        \n",
    "        self.Beta1.assign(self.Weight*Loss1/Loss2)\n",
    "        self.Beta2.assign(self.Weight*Loss2/Loss1)   \n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName1+' : ', self.Beta1.numpy())\n",
    "            print(self.BetaName2+' : ', self.Beta2.numpy())   \n",
    "            \n",
    "        \n",
    "def ReName (layer, name):\n",
    "    return Lambda(lambda x: x, name=name)(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(SigDim, LatDim= 2, Type = '', MaskingRate = 0.025, NoiseStd = 0.002, MaskStd = 0.1, ReparaStd = 0.1 , Reparam = False):\n",
    "\n",
    "    InpL = Input(shape=(SigDim,))\n",
    "    InpFrame = tf.signal.frame(InpL, 100, 100)\n",
    "\n",
    "    if Reparam:\n",
    "        InpRegul = GaussianNoise(stddev=NoiseStd)(InpFrame, training=Reparam)\n",
    "        MaskVec, NoisVec = MaskingGen(InpRegul, MaskingRate, MaskStd)\n",
    "        EncInp = Masking(mask_value=0.)(InpRegul * MaskVec )\n",
    "        EncOut = InpRegul + NoisVec\n",
    "    else:\n",
    "        EncInp, EncOut = InpFrame, InpFrame\n",
    "\n",
    "    Encoder = Dense(50, activation='relu')(InpFrame)\n",
    "    Encoder = Bidirectional(GRU(30, return_sequences=True))(Encoder)\n",
    "    Encoder = Bidirectional(GRU(30, return_sequences=False))(Encoder)\n",
    "    Encoder = Dense(50, activation='relu')(Encoder)\n",
    "    Encoder = Dense(30, activation='relu')(Encoder)\n",
    "    Encoder = Dense(15, activation='relu')(Encoder)\n",
    "\n",
    "    Z_Mean = Dense(LatDim, activation='linear')(Encoder)\n",
    "    Z_Log_Sigma = Dense(LatDim, activation='relu')(Encoder)\n",
    "    Z_Log_Sigma = ReName(Z_Log_Sigma,'Z_Log_Sigma_'+Type)\n",
    "\n",
    "    \n",
    "    # Reparameterization Trick for sampling from Guassian distribution\n",
    "    Epsilon = tf.random.normal(shape=(tf.shape(Z_Mean)[0], Z_Mean.shape[1]), mean=0., stddev=ReparaStd)\n",
    "\n",
    "    if Reparam==False:\n",
    "        Epsilon = Epsilon * 0\n",
    "\n",
    "    Z_Mean = Z_Mean + tf.exp(0.5 * Z_Log_Sigma) * Epsilon\n",
    "    Z_Mean = ReName(Z_Mean,'Z_Mean_'+Type)\n",
    "    \n",
    "    FCs =   Dense(6, activation='relu')(Z_Mean)\n",
    "    FCs =   Dense(6, activation='sigmoid')(FCs)\n",
    "    \n",
    "    \n",
    "    # Reparameterization Trick for sampling from Uniformly distribution; ϵ∼U(0,1) \n",
    "    FCs = tf.clip_by_value(FCs, 1e-7, 1-1e-7)\n",
    "    Epsilon = tf.random.uniform(shape=(tf.shape(FCs)[0], FCs.shape[1]))\n",
    "    Epsilon = tf.clip_by_value(Epsilon, 1e-7, 1-1e-7)\n",
    "    \n",
    "    LogEps = tf.math.log(Epsilon)\n",
    "    LogNegEps = tf.math.log(1 - Epsilon)\n",
    "    \n",
    "    LogTheta = tf.math.log(FCs)\n",
    "    LogNegTheta = tf.math.log(1-FCs)\n",
    "    \n",
    "    \n",
    "    FCs = tf.math.sigmoid(LogEps - LogNegEps + LogTheta - LogNegTheta)\n",
    "    FCs = tf.clip_by_value(FCs, 1e-7, 1-1e-7)\n",
    "    FCs = ReName(FCs, 'FCs')\n",
    "    \n",
    "    return [InpL], [Flatten()(EncOut), Z_Mean, FCs]\n",
    "\n",
    "\n",
    "\n",
    "def FeatExtractor(Inps, LatDim= 2, FiltLenList = [301, 301, 301, 301, 301, 301] ):\n",
    "    \n",
    "    EncReInp, InpZ, FCs = Inps\n",
    "    \n",
    "    H_F, L_F, HH_F, HL_F, LH_F, LL_F = tf.split(FCs, 6, axis=1)\n",
    "    \n",
    "\n",
    "    ### Filtering level 1 -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_H = GenHighFilter(H_F,  N=FiltLenList[0])\n",
    "    To_L = GenLowFilter(L_F, N=FiltLenList[1])\n",
    "\n",
    "    ## Perform signal filtering level 1\n",
    "    InpFrame =  tf.signal.frame(EncReInp, To_H.shape[-1], 1)\n",
    "    Sig_H = tf.reduce_sum(InpFrame*To_H[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_H = ReName(Sig_H, 'Sig_H_Ext')\n",
    "\n",
    "    InpFrame =  tf.signal.frame(EncReInp, To_L.shape[-1], 1)\n",
    "    Sig_L = tf.reduce_sum(InpFrame*To_L[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_L = ReName(Sig_L, 'Sig_L_Ext')\n",
    "\n",
    "\n",
    "\n",
    "    ### Filtering level HH and HL (from Sig_H) -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_HH = GenHighFilter(HH_F, N=FiltLenList[2])\n",
    "    To_HL = GenLowFilter(HL_F, N=FiltLenList[3])\n",
    "\n",
    "    ## Perform signal filtering level 2\n",
    "    Frame_H =  tf.signal.frame(Sig_H[:,:,0], To_HH.shape[-1], 1)\n",
    "    Sig_HH = tf.reduce_sum(Frame_H*To_HH[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_HH = ReName(Sig_HH, 'Sig_HH_Ext')\n",
    "\n",
    "    Frame_H =  tf.signal.frame(Sig_H[:,:,0], To_HL.shape[-1], 1)\n",
    "    Sig_HL = tf.reduce_sum(Frame_H*To_HL[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_HL = ReName(Sig_HL, 'Sig_HL_Ext')\n",
    "\n",
    "\n",
    "    ### Filtering level LH and LL (from Sig_L) -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_LH = GenHighFilter(LH_F,  N=FiltLenList[4])\n",
    "    To_LL = GenLowFilter(LL_F,  N=FiltLenList[5])\n",
    "\n",
    "    ## Perform signal filtering level 2\n",
    "    Frame_L =  tf.signal.frame(Sig_L[:,:,0], To_LH.shape[-1], 1)\n",
    "    Sig_LH = tf.reduce_sum(Frame_L*To_LH[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_LH = ReName(Sig_LH, 'Sig_LH_Ext')\n",
    "\n",
    "    Frame_L =  tf.signal.frame(Sig_L[:,:,0], To_LL.shape[-1], 1)\n",
    "    Sig_LL = tf.reduce_sum(Frame_L*To_LL[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_LL = ReName(Sig_LL, 'Sig_LL_Ext')\n",
    "\n",
    "    \n",
    "    return [Flatten()(Sig_HH), Flatten()(Sig_HL), Flatten()(Sig_LH), Flatten()(Sig_LL)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FeatGenerator (Inp_lat):\n",
    "    \n",
    "    Inp_lat = tf.concat(Inp_lat, axis=-1)\n",
    "   \n",
    "    Dec_Sig_HH = Dense(10, activation='relu')(Inp_lat)\n",
    "    Dec_Sig_HH = Dense(20, activation='relu')(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(30, activation='relu')(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(50, activation='relu')(Dec_Sig_HH)\n",
    "\n",
    "    Dec_Sig_HH = RepeatVector(10 )(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(40,'tanh')(Dec_Sig_HH)\n",
    "    Sig_HH= Flatten(name='Sig_HH_Gen')(Dec_Sig_HH)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_HL = Dense(10, activation='relu')(Inp_lat)\n",
    "    Dec_Sig_HL = Dense(20, activation='relu')(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(30, activation='relu')(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(50, activation='relu')(Dec_Sig_HL)\n",
    "\n",
    "    Dec_Sig_HL = RepeatVector(10 )(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(40,'tanh')(Dec_Sig_HL)\n",
    "    Sig_HL= Flatten(name='Sig_HL_Gen')(Dec_Sig_HL)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_LH = Dense(10, activation='relu')(Inp_lat)\n",
    "    Dec_Sig_LH = Dense(20, activation='relu')(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(30, activation='relu')(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(50, activation='relu')(Dec_Sig_LH)\n",
    "\n",
    "    Dec_Sig_LH = RepeatVector(10 )(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(40,'tanh')(Dec_Sig_LH)\n",
    "    Sig_LH= Flatten(name='Sig_LH_Gen')(Dec_Sig_LH)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_LL = Dense(10, activation='relu')(Inp_lat)\n",
    "    Dec_Sig_LL = Dense(20, activation='relu')(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(30, activation='relu')(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(50, activation='relu')(Dec_Sig_LL)\n",
    "\n",
    "    Dec_Sig_LL = RepeatVector(10 )(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(40,'tanh')(Dec_Sig_LL)\n",
    "    Sig_LL= Flatten(name='Sig_LL_Gen')(Dec_Sig_LL)\n",
    "    \n",
    "    return  Sig_HH, Sig_HL, Sig_LH, Sig_LL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Reconstructor(Inps ):\n",
    "    Sigs, FCs = Inps\n",
    "    Sig_HH, Sig_HL, Sig_LH, Sig_LL = Sigs\n",
    "\n",
    "    ## GRU NET -------------------------------------------------------------------\n",
    "    Dec_Sig_HH = Reshape((-1, 100))(Sig_HH)\n",
    "    Dec_Sig_HL = Reshape((-1, 100))(Sig_HL)\n",
    "    Dec_Sig_LH = Reshape((-1, 100))(Sig_LH)\n",
    "    Dec_Sig_LL = Reshape((-1, 100))(Sig_LL)\n",
    "\n",
    "    Dec_Sig_HH = Bidirectional(GRU(5), name='Dec_Sig_HH')(Dec_Sig_HH)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(5), name='Dec_Sig_HL')(Dec_Sig_HL)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(5), name='Dec_Sig_LH')(Dec_Sig_LH)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(5), name='Dec_Sig_LL')(Dec_Sig_LL)\n",
    "\n",
    "    Decoder = tf.concat([ Dec_Sig_HH, Dec_Sig_HL, Dec_Sig_LH, Dec_Sig_LL, FCs], axis=1)\n",
    "    Decoder = RepeatVector((SigDim//100) )(Decoder)\n",
    "    Decoder = Bidirectional(GRU(50, return_sequences=True))(Decoder)\n",
    "    Decoder = Dense(100, activation='relu')(Decoder)\n",
    "    DecOut = Dense(100, activation='sigmoid')(Decoder)\n",
    "    DecOut = Reshape((SigDim,),name='Out')(DecOut)\n",
    "\n",
    "    \n",
    "    return DecOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder - FeatExtractor\n",
    "EncInp, EncOut = Encoder(SigDim=SigDim, LatDim= LatDim, Type = 'Tr', MaskingRate = MaskingRate, NoiseStd = NoiseStd, MaskStd = MaskStd, ReparaStd = ReparaStd, Reparam=True)\n",
    "FeatExtOut = FeatExtractor(EncOut)\n",
    "\n",
    "### Encoder - FeatGenerator - Reconstruction\n",
    "FeatGenOut = FeatGenerator(EncOut[1:])\n",
    "FeatGenOut = ReName(FeatGenOut, 'FeatGenOut')\n",
    "\n",
    "ReconOut = Reconstructor([FeatGenOut, EncOut[2]])\n",
    "ReconOut = ReName(ReconOut, 'ReconOut')\n",
    "\n",
    "### Define the total model\n",
    "SigBandRepModel = Model(EncInp, ReconOut)\n",
    "\n",
    "SigBandRepModel.add_weight(initializer=tf.constant_initializer(value=1.0), name='Beta_Rec', trainable=False)\n",
    "SigBandRepModel.add_weight(initializer=tf.constant_initializer(value=1.0), name='Beta_Feat', trainable=False)\n",
    "Beta_Rec = [i for i in SigBandRepModel.variables if 'Beta_Rec' in i.name][0]\n",
    "Beta_Feat = [i for i in SigBandRepModel.variables if 'Beta_Feat' in i.name][0]\n",
    "\n",
    "\n",
    "### Weight controller; Apply beta and capacity \n",
    "Capacity = 0.1 # 0.1 0.05\n",
    "Beta_Z = Lossweight(name='Beta_Z')(FeatGenOut)\n",
    "Beta_Fc = Lossweight(name='Beta_Fc')(FeatGenOut)\n",
    "\n",
    "\n",
    "### Custom loss for Reconstruction\n",
    "def RECMSE (y_true, y_pred):\n",
    "    \n",
    "    RecMSE = tf.losses.mse(y_true, y_pred)\n",
    "    RecMSE *= Beta_Rec\n",
    "    \n",
    "    return RecMSE\n",
    "\n",
    "\n",
    "### Adding the FeatRecLoss; It allows connection between the extractor and generator\n",
    "MSE = tf.keras.losses.MeanSquaredError()\n",
    "FeatRecLoss= MSE(tf.concat(FeatGenOut, axis=-1), tf.concat(FeatExtOut, axis=-1))\n",
    "\n",
    "SigBandRepModel.add_loss(FeatRecLoss * Beta_Feat.numpy() )\n",
    "SigBandRepModel.add_metric(FeatRecLoss, 'FeatRecLoss')\n",
    "\n",
    "### KL Divergence for p(Z) vs q(Z)\n",
    "Z_Sampled, Z_Log_Sigma = SigBandRepModel.get_layer('Z_Mean_Tr').output, SigBandRepModel.get_layer('Z_Log_Sigma_Tr').output\n",
    "kl_Loss_Z = 0.5 * tf.reduce_sum( Z_Sampled**2  +  tf.exp(Z_Log_Sigma)- Z_Log_Sigma-1, axis=1)    \n",
    "kl_Loss_Z = tf.reduce_mean(kl_Loss_Z )\n",
    "kl_Loss_Z = Beta_Z * tf.abs(kl_Loss_Z - Capacity)\n",
    "\n",
    "### KL Divergence for p(FCs) vs q(FCs)\n",
    "BernP = 0.5 # hyperparameter\n",
    "FCs = SigBandRepModel.get_layer('FCs').output\n",
    "kl_Loss_FC = tf.math.log(FCs) - tf.math.log(BernP) + tf.math.log(1-FCs) - tf.math.log(1-BernP) \n",
    "kl_Loss_FC = tf.reduce_mean(-kl_Loss_FC )\n",
    "kl_Loss_FC = Beta_Fc * tf.abs(kl_Loss_FC - 0.6)\n",
    "\n",
    "SigBandRepModel.add_loss(kl_Loss_Z )\n",
    "SigBandRepModel.add_metric(kl_Loss_Z, 'kl_Loss_Z')\n",
    "\n",
    "SigBandRepModel.add_loss(kl_Loss_FC )\n",
    "SigBandRepModel.add_metric(kl_Loss_FC, 'kl_Loss_FC')\n",
    "\n",
    "\n",
    "## Model Compile\n",
    "SigBandRepModel.compile(loss=RECMSE, optimizer='adam', metrics={\"ReconOut\":'mse'}) \n",
    "\n",
    "### Loss and KLD_Beta controller\n",
    "KLD_Beta_Z = KLAnneal(TargetLossName=['val_FeatRecLoss', 'val_mse'], Threshold=0.0015, MaxBeta=0.5 , BetaName='Beta_Z', AnnealEpoch=500, verbose=2)\n",
    "KLD_Beta_Fc = KLAnneal(TargetLossName=['val_FeatRecLoss', 'val_mse'], Threshold=0.0015, MaxBeta=0.5 , BetaName='Beta_Fc', AnnealEpoch=500, verbose=1)\n",
    "RelLoss = RelLossWeight('val_mse', 'val_FeatRecLoss', Beta_Rec, Beta_Feat, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta_Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "92/92 [==============================] - 73s 571ms/step - loss: 0.0163 - mse: 0.0130 - FeatRecLoss: 0.0030 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0112 - val_mse: 0.0086 - val_FeatRecLoss: 0.0026 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_mse improved from inf to 0.00858, saving model to ./Results\\SigBandRepModel_DyLoss.hdf5\n",
      "TargetLoss :  0.008578947745263577\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  3.331354\n",
      "Beta2 :  0.30017826\n",
      "Epoch 2/700\n",
      "92/92 [==============================] - 47s 506ms/step - loss: 0.0309 - mse: 0.0085 - FeatRecLoss: 0.0024 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0306 - val_mse: 0.0085 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_mse improved from 0.00858 to 0.00848, saving model to ./Results\\SigBandRepModel_DyLoss.hdf5\n",
      "TargetLoss :  0.008481995202600956\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  3.668716\n",
      "Beta2 :  0.27257493\n",
      "Epoch 3/700\n",
      "92/92 [==============================] - 47s 508ms/step - loss: 0.0333 - mse: 0.0085 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0333 - val_mse: 0.0085 - val_FeatRecLoss: 0.0022 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_mse improved from 0.00848 to 0.00847, saving model to ./Results\\SigBandRepModel_DyLoss.hdf5\n",
      "TargetLoss :  0.008473310619592667\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  3.868067\n",
      "Beta2 :  0.25852707\n",
      "Epoch 4/700\n",
      "92/92 [==============================] - 49s 539ms/step - loss: 0.0349 - mse: 0.0085 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0348 - val_mse: 0.0084 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_mse improved from 0.00847 to 0.00844, saving model to ./Results\\SigBandRepModel_DyLoss.hdf5\n",
      "TargetLoss :  0.008440990000963211\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  3.9419322\n",
      "Beta2 :  0.2536827\n",
      "Epoch 5/700\n",
      "92/92 [==============================] - 55s 595ms/step - loss: 0.0354 - mse: 0.0084 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0354 - val_mse: 0.0084 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_mse improved from 0.00844 to 0.00843, saving model to ./Results\\SigBandRepModel_DyLoss.hdf5\n",
      "TargetLoss :  0.00843087024986744\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  3.9827309\n",
      "Beta2 :  0.251084\n",
      "Epoch 6/700\n",
      "92/92 [==============================] - 47s 515ms/step - loss: 0.0356 - mse: 0.0084 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0357 - val_mse: 0.0084 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_mse did not improve from 0.00843\n",
      "TargetLoss :  0.008437808603048325\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  4.033325\n",
      "Beta2 :  0.24793439\n",
      "Epoch 7/700\n",
      "92/92 [==============================] - 48s 520ms/step - loss: 0.0360 - mse: 0.0084 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0361 - val_mse: 0.0084 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_mse did not improve from 0.00843\n",
      "TargetLoss :  0.008431103080511093\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  4.1062083\n",
      "Beta2 :  0.24353367\n",
      "Epoch 8/700\n",
      "92/92 [==============================] - 48s 528ms/step - loss: 0.0366 - mse: 0.0084 - FeatRecLoss: 0.0020 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0367 - val_mse: 0.0084 - val_FeatRecLoss: 0.0020 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_mse did not improve from 0.00843\n",
      "TargetLoss :  0.008438828401267529\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  4.1757865\n",
      "Beta2 :  0.23947583\n",
      "Epoch 9/700\n",
      "92/92 [==============================] - 48s 522ms/step - loss: 0.0371 - mse: 0.0084 - FeatRecLoss: 0.0020 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0371 - val_mse: 0.0084 - val_FeatRecLoss: 0.0020 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_mse improved from 0.00843 to 0.00841, saving model to ./Results\\SigBandRepModel_DyLoss.hdf5\n",
      "TargetLoss :  0.008413131348788738\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  4.213546\n",
      "Beta2 :  0.23732981\n",
      "Epoch 10/700\n",
      "92/92 [==============================] - 50s 546ms/step - loss: 0.0374 - mse: 0.0084 - FeatRecLoss: 0.0020 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0375 - val_mse: 0.0084 - val_FeatRecLoss: 0.0020 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_mse did not improve from 0.00841\n",
      "TargetLoss :  0.008417084813117981\n",
      "Beta_Z :  0.0\n",
      "Beta_Fc :  0.0\n",
      "Beta1 :  4.229276\n",
      "Beta2 :  0.23644708\n",
      "Epoch 11/700\n",
      "81/92 [=========================>....] - ETA: 5s - loss: 0.0376 - mse: 0.0084 - FeatRecLoss: 0.0020 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "SigBandRepModel.load_weights(ModelSaveSameName)\n",
    "ModelSave = ModelCheckpoint(filepath=ModelSaveSameName, monitor='val_mse', verbose=1, save_best_only=True )\n",
    "\n",
    "SigBandRepModel.fit(DATA[:], DATA[:], batch_size=3500, epochs=700, shuffle=True, validation_split=0.2, callbacks=[EarlyStop, ModelSave, KLD_Beta_Z, KLD_Beta_Fc, RelLoss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SigBandRepModel.load_weights(ModelSaveSameName)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
