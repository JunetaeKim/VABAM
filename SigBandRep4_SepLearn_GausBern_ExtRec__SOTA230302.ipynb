{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Masking, Reshape, Flatten, RepeatVector, TimeDistributed, Bidirectional, Activation, GaussianNoise, Lambda, LSTM\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from Models.FeatExtModels_NoKaiser import *\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.load('./Data/AsanTRSet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Env setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Results/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "### Model checkpoint\n",
    "ModelSaveSameName = save_path+'SigBandRepModel_ExtRec_0302.hdf5'\n",
    "ModelSave = ModelCheckpoint(filepath=ModelSaveSameName, monitor='val_mse', verbose=1, save_best_only=True )\n",
    "\n",
    "### Model Early stop\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', patience=500)\n",
    "\n",
    "LatDim = 3\n",
    "SigDim = DATA.shape[1]\n",
    "MaskingRate = 0.02\n",
    "NoiseStd = 0.002\n",
    "MaskStd = 0.1\n",
    "ReparaStd = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, LossName1, LossName2, BetaName1, BetaName2, verbose=1):\n",
    "                \n",
    "        self.LossName1 = LossName1\n",
    "        self.LossName2 = LossName2\n",
    "        self.BetaName1 = BetaName1\n",
    "        self.BetaName2 = BetaName2\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Loss1 = logs[self.LossName1] \n",
    "        Loss2 = logs[self.LossName2] \n",
    "        \n",
    "        Beta1_idx = [num for num, i in enumerate(self.model.variables) if self.BetaName1 in i.name][0]\n",
    "        Beta2_idx = [num for num, i in enumerate(self.model.variables) if self.BetaName2 in i.name][0]\n",
    "        \n",
    "        self.model.variables[Beta1_idx].assign(Loss1/Loss2)\n",
    "        self.model.variables[Beta2_idx].assign(Loss2/Loss1)   \n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName1+' : ', self.model.variables[Beta1_idx])\n",
    "            print(self.BetaName2+' : ', self.model.variables[Beta2_idx])        \n",
    " \n",
    "\n",
    "# Define the KL annealing callback function\n",
    "class KLCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold, MaxBeta, BetaName, AnnealEpoch=100):\n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.BetaName = BetaName\n",
    "        self.MaxBeta = MaxBeta\n",
    "        self.AnnealStart = 0\n",
    "        self.AnnealEpoch = AnnealEpoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = logs['val_'+self.TargetLossName]\n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            self.AnnealStart = 0\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], 0.)\n",
    "        else: \n",
    "            self.AnnealStart += 1\n",
    "            Beta = (self.AnnealStart) / self.AnnealEpoch * self.MaxBeta\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], Beta)\n",
    "        \n",
    "        print(self.model.get_layer(self.BetaName).variables[0])\n",
    "'''\n",
    "\n",
    "'''\n",
    "class KLAnneal(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold,  BetaName, MaxBeta=0.1, MinBeta=1e-5, AnnealEpoch=100, UnderLimit=0., verbose=1):\n",
    "        \n",
    "        if type(TargetLossName) != list:\n",
    "            TargetLossName = [TargetLossName]\n",
    "        \n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.BetaName = BetaName\n",
    "        self.AnnealIdx = 0\n",
    "        self.verbose = verbose \n",
    "        self.Beta =  np.concatenate([np.array([UnderLimit]), np.linspace(start=MinBeta, stop=MaxBeta, num=AnnealEpoch )])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = max([logs[i] for i in self.TargetLossName]) \n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            #self.AnnealIdx -= 1\n",
    "            #self.AnnealIdx = np.maximum(self.AnnealIdx, 0)\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], self.Beta[self.AnnealIdx])\n",
    "        else: \n",
    "            self.AnnealIdx += 1\n",
    "            self.AnnealIdx = np.minimum(self.AnnealIdx, len(self.Beta)-1)\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], self.Beta[self.AnnealIdx])\n",
    "        \n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "        elif self.verbose==2:\n",
    "            print('TargetLoss : ', TargetLoss)\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "'''\n",
    "\n",
    "'''\n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, LossName1, LossName2, Beta1, Beta2, WeightB1 =1., WeightB2 =1., BetaName1 ='Beta1' , BetaName2 ='Beta2' , verbose=1):\n",
    "                \n",
    "        self.LossName1 = LossName1\n",
    "        self.LossName2 = LossName2\n",
    "        self.Beta1 = Beta1\n",
    "        self.Beta2 = Beta2\n",
    "        self.BetaName1 = BetaName1\n",
    "        self.BetaName2 = BetaName2\n",
    "        self.verbose = verbose\n",
    "        self.WeightB1 = WeightB1\n",
    "        self.WeightB2 = WeightB2\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Loss1 = logs[self.LossName1] \n",
    "        Loss2 = logs[self.LossName2] \n",
    "        \n",
    "        Beta1 = tf.maximum(Loss1/Loss2, 1.) * self.WeightB1\n",
    "        Beta2 = tf.maximum(Loss2/Loss1, 1.) * self.WeightB2\n",
    "        \n",
    "        self.Beta1.assign(Beta1)\n",
    "        self.Beta2.assign(Beta2)   \n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName1+' : ', self.Beta1.numpy())\n",
    "            print(self.BetaName2+' : ', self.Beta2.numpy())   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class KLAnneal(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold,  Beta, BetaName ='Beta', MaxBeta=0.1, MinBeta=1e-5, AnnealEpoch=100, UnderLimit=0., verbose=1):\n",
    "        \n",
    "        if type(TargetLossName) != list:\n",
    "            TargetLossName = [TargetLossName]\n",
    "        \n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.Beta = Beta\n",
    "        self.BetaName = BetaName\n",
    "        self.AnnealIdx = 0\n",
    "        self.verbose = verbose \n",
    "        self.BetaValue =  np.concatenate([np.array([UnderLimit]), np.linspace(start=MinBeta, stop=MaxBeta, num=AnnealEpoch )])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = max([logs[i] for i in self.TargetLossName]) \n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            #self.AnnealIdx -= 1\n",
    "            #self.AnnealIdx = np.maximum(self.AnnealIdx, 0)\n",
    "            self.Beta.assign(self.BetaValue[self.AnnealIdx])\n",
    "        else: \n",
    "            self.AnnealIdx += 1\n",
    "            self.AnnealIdx = np.minimum(self.AnnealIdx, len(self.BetaValue)-1)\n",
    "            self.Beta.assign(self.BetaValue[self.AnnealIdx])\n",
    "        \n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName+' : ' ,self.Beta.numpy())\n",
    "        elif self.verbose==2:\n",
    "            print('TargetLoss : ', TargetLoss)\n",
    "            print(self.BetaName+' : ' ,self.Beta.numpy())\n",
    "            \n",
    "            \n",
    "       \n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, BetaList, BetaWeight, MinLimit , MaxLimit , verbose=1):\n",
    "                \n",
    "        self.BetaList = BetaList\n",
    "        self.BetaWeight = BetaWeight\n",
    "        self.MinLimit = MinLimit\n",
    "        self.MaxLimit = MaxLimit\n",
    "        self.verbose = verbose\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Losses = np.array([logs[i] for i in self.BetaList.keys()])\n",
    "        Losses = np.maximum(1e-7, Losses)\n",
    "        RelWeights = Losses / np.min(Losses)\n",
    "        RelWeights = {LossName:RelWeights[num] for num, LossName in enumerate (self.BetaList.keys())}\n",
    "\n",
    "        for name, beta in self.BetaList.items():\n",
    "            \n",
    "            Value = np.clip(RelWeights[name] * self.BetaWeight[name], self.MinLimit[name], self.MaxLimit[name])\n",
    "            beta.assign(Value)\n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(RelWeights)\n",
    "            for key, beta in self.BetaList.items():\n",
    "                print(key, ': ', beta.numpy())\n",
    "                \n",
    "RelLossDic = {'val_mse':'Beta_Rec', 'val_FeatRecLoss':'Beta_Feat', 'val_kl_Loss_Z':'Beta_Z', 'val_kl_Loss_FC':'Beta_Fc'}\n",
    "WeightDic = {'val_mse':10., 'val_FeatRecLoss':100., 'val_kl_Loss_Z':1., 'val_kl_Loss_FC':1.}\n",
    "MinLimit = {'val_mse':1., 'val_FeatRecLoss':1., 'val_kl_Loss_Z':0.01, 'val_kl_Loss_FC':0.01}\n",
    "MaxLimit = {'val_mse':200., 'val_FeatRecLoss':200., 'val_kl_Loss_Z':0.1, 'val_kl_Loss_FC':0.1}\n",
    "RelLoss = RelLossWeight(BetaList=RelLossDic, BetaWeight= WeightDic, MinLimit= MinLimit, MaxLimit = MaxLimit )\n",
    "           \n",
    "'''        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def ReName (layer, name):\n",
    "    return Lambda(lambda x: x, name=name)(layer)\n",
    "\n",
    "def ParaFilters (layer, name=''):\n",
    "    Fc = Dense(1, activation='sigmoid')(layer)\n",
    "    Fc = tf.clip_by_value(Fc, 1e-7, 1-1e-7)\n",
    "    \n",
    "    # Reparameterization Trick for sampling from Uniformly distribution; ϵ∼U(0,1) \n",
    "    Epsilon = tf.random.uniform(shape=(tf.shape(Fc)[0], Fc.shape[1]))\n",
    "    Epsilon = tf.clip_by_value(Epsilon, 1e-7, 1-1e-7)\n",
    "\n",
    "    LogEps = tf.math.log(Epsilon)\n",
    "    LogNegEps = tf.math.log(1 - Epsilon)\n",
    "    \n",
    "    LogTheta = tf.math.log(Fc)\n",
    "    LogNegTheta = tf.math.log(1-Fc)\n",
    "\n",
    "    Fc = tf.math.sigmoid(LogEps - LogNegEps + LogTheta - LogNegTheta)\n",
    "    Fc = tf.clip_by_value(Fc, 1e-7, 1-1e-7)\n",
    "    Fc = ReName(Fc, name)\n",
    "    \n",
    "    return Fc \n",
    "\n",
    "\n",
    "class Lossweight(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, InitVal = 0., name='Lossweight'):\n",
    "        super(Lossweight, self).__init__(name=name)\n",
    "        self.InitVal = InitVal\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.GenVec = tf.Variable(self.InitVal, trainable=False)\n",
    "    \n",
    "    def call(self, input):\n",
    "\n",
    "        return self.GenVec\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Lossweight, self).get_config()\n",
    "        config.update({ 'InitVal': self.InitVal })\n",
    "        return config\n",
    "    \n",
    "\n",
    "class RandFCs(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super(RandFCs, self).__init__(name='FCs')\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.GenVec = tf.Variable(tf.random.uniform(shape=(1,6)), trainable=False)\n",
    "    \n",
    "    def call(self, input):\n",
    "        return tf.tile(self.GenVec , (tf.shape(input)[0],1))\n",
    "    \n",
    "\n",
    "\n",
    "class KLAnneal(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, TargetLossName, Threshold,  BetaName, MaxBeta=0.1, MinBeta=1e-5, AnnealEpoch=100, UnderLimit=0., verbose=1):\n",
    "        \n",
    "        if type(TargetLossName) != list:\n",
    "            TargetLossName = [TargetLossName]\n",
    "        \n",
    "        self.TargetLossName = TargetLossName\n",
    "        self.Threshold = Threshold\n",
    "        self.BetaName = BetaName\n",
    "        self.AnnealIdx = 0\n",
    "        self.verbose = verbose \n",
    "        self.Beta =  np.concatenate([np.array([UnderLimit]), np.linspace(start=MinBeta, stop=MaxBeta, num=AnnealEpoch )])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        TargetLoss = max([logs[i] for i in self.TargetLossName]) \n",
    "        \n",
    "        if TargetLoss > self.Threshold:\n",
    "            \n",
    "            self.AnnealIdx -= 1\n",
    "            self.AnnealIdx = np.maximum(self.AnnealIdx, 0)\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], self.Beta[self.AnnealIdx])\n",
    "        else: \n",
    "            self.AnnealIdx += 1\n",
    "            self.AnnealIdx = np.minimum(self.AnnealIdx, len(self.Beta)-1)\n",
    "            K.set_value(self.model.get_layer(self.BetaName).variables[0], self.Beta[self.AnnealIdx])\n",
    "        \n",
    "        if self.verbose==1:\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "        elif self.verbose==2:\n",
    "            print('TargetLoss : ', TargetLoss)\n",
    "            print(self.BetaName+' : ' ,self.model.get_layer(self.BetaName).variables[0].numpy())\n",
    "            \n",
    "          \n",
    "\n",
    "class RelLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, BetaList, LossScaling, MinLimit , MaxLimit , verbose=1):\n",
    "                \n",
    "        self.BetaList = BetaList\n",
    "        self.LossScaling = LossScaling\n",
    "        self.MinLimit = MinLimit\n",
    "        self.MaxLimit = MaxLimit\n",
    "        self.verbose = verbose\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        Losses = np.array([logs[i] for i in self.BetaList.keys()])\n",
    "        Losses = np.maximum(1e-7, Losses)\n",
    "        RelWeights = Losses / np.min(Losses)\n",
    "        RelWeights = {loss:RelWeights[num] * self.LossScaling[loss] for num, loss in enumerate (self.BetaList.keys())}\n",
    "\n",
    "        for loss, beta in self.BetaList.items():\n",
    "            \n",
    "            Value = np.clip(RelWeights[loss] , self.MinLimit[beta], self.MaxLimit[beta])\n",
    "            K.set_value(self.model.get_layer(beta).variables[0], Value)\n",
    "\n",
    "        if self.verbose==1:\n",
    "            print(RelWeights)\n",
    "            for key, beta in self.BetaList.items():\n",
    "                print(beta, ': ', self.model.get_layer(beta).variables[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(SigDim, LatDim= 2, Type = '', MaskingRate = 0.025, NoiseStd = 0.002, MaskStd = 0.1, ReparaStd = 0.1 , Reparam = False):\n",
    "\n",
    "    InpL = Input(shape=(SigDim,))\n",
    "    InpFrame = tf.signal.frame(InpL, 100, 100)\n",
    "\n",
    "    if Reparam:\n",
    "        InpRegul = GaussianNoise(stddev=NoiseStd)(InpFrame, training=Reparam)\n",
    "        MaskVec, NoisVec = MaskingGen(InpRegul, MaskingRate, MaskStd)\n",
    "        EncInp = Masking(mask_value=0.)(InpRegul * MaskVec )\n",
    "        EncOut = InpRegul + NoisVec\n",
    "    else:\n",
    "        EncInp, EncOut = InpFrame, InpFrame\n",
    "\n",
    "    Encoder = Dense(50, activation='relu')(InpFrame)\n",
    "    Encoder = Bidirectional(GRU(30, return_sequences=True))(Encoder)\n",
    "    Encoder = Bidirectional(GRU(30, return_sequences=False))(Encoder)\n",
    "    Encoder = Dense(50, activation='relu')(Encoder)\n",
    "    Encoder = Dense(30, activation='relu')(Encoder)\n",
    "    Encoder = Dense(15, activation='relu')(Encoder)\n",
    "\n",
    "    Z_Mean = Dense(LatDim, activation='linear')(Encoder)\n",
    "    Z_Log_Sigma = Dense(LatDim, activation='relu')(Encoder)\n",
    "    Z_Log_Sigma = ReName(Z_Log_Sigma,'Z_Log_Sigma_'+Type)\n",
    "\n",
    "    \n",
    "    # Reparameterization Trick for sampling from Guassian distribution\n",
    "    Epsilon = tf.random.normal(shape=(tf.shape(Z_Mean)[0], Z_Mean.shape[1]), mean=0., stddev=ReparaStd)\n",
    "\n",
    "    if Reparam==False:\n",
    "        Epsilon = Epsilon * 0\n",
    "\n",
    "    Z_Mean = Z_Mean + tf.exp(0.5 * Z_Log_Sigma) * Epsilon\n",
    "    Z_Mean = ReName(Z_Mean,'Z_Mean_'+Type)\n",
    "    \n",
    "    FCs =   Dense(6, activation='relu')(Encoder)\n",
    "    FCs =   Dense(6, activation='sigmoid')(FCs)\n",
    "    \n",
    "    \n",
    "    # Reparameterization Trick for sampling from Uniformly distribution; ϵ∼U(0,1) \n",
    "    FCs = tf.clip_by_value(FCs, 1e-7, 1-1e-7)\n",
    "    Epsilon = tf.random.uniform(shape=(tf.shape(FCs)[0], FCs.shape[1]))\n",
    "    Epsilon = tf.clip_by_value(Epsilon, 1e-7, 1-1e-7)\n",
    "    \n",
    "    LogEps = tf.math.log(Epsilon)\n",
    "    LogNegEps = tf.math.log(1 - Epsilon)\n",
    "    \n",
    "    LogTheta = tf.math.log(FCs)\n",
    "    LogNegTheta = tf.math.log(1-FCs)\n",
    "    \n",
    "    \n",
    "    FCs = tf.math.sigmoid(LogEps - LogNegEps + LogTheta - LogNegTheta)\n",
    "    FCs = tf.clip_by_value(FCs, 1e-7, 1-1e-7)\n",
    "    FCs = ReName(FCs, 'FCs')\n",
    "    \n",
    "    return [InpL], [Flatten()(EncOut), Z_Mean, FCs]\n",
    "\n",
    "\n",
    "\n",
    "def FeatExtractor(Inps, LatDim= 2, FiltLenList = [301, 301, 301, 301, 301, 301] ):\n",
    "    \n",
    "    EncReInp, InpZ, FCs = Inps\n",
    "    \n",
    "    H_F, L_F, HH_F, HL_F, LH_F, LL_F = tf.split(FCs, 6, axis=1)\n",
    "    \n",
    "\n",
    "    ### Filtering level 1 -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_H = GenHighFilter(H_F,  N=FiltLenList[0])\n",
    "    To_L = GenLowFilter(L_F, N=FiltLenList[1])\n",
    "\n",
    "    ## Perform signal filtering level 1\n",
    "    InpFrame =  tf.signal.frame(EncReInp, To_H.shape[-1], 1)\n",
    "    Sig_H = tf.reduce_sum(InpFrame*To_H[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_H = ReName(Sig_H, 'Sig_H_Ext')\n",
    "\n",
    "    InpFrame =  tf.signal.frame(EncReInp, To_L.shape[-1], 1)\n",
    "    Sig_L = tf.reduce_sum(InpFrame*To_L[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_L = ReName(Sig_L, 'Sig_L_Ext')\n",
    "\n",
    "\n",
    "\n",
    "    ### Filtering level HH and HL (from Sig_H) -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_HH = GenHighFilter(HH_F, N=FiltLenList[2])\n",
    "    To_HL = GenLowFilter(HL_F, N=FiltLenList[3])\n",
    "\n",
    "    ## Perform signal filtering level 2\n",
    "    Frame_H =  tf.signal.frame(Sig_H[:,:,0], To_HH.shape[-1], 1)\n",
    "    Sig_HH = tf.reduce_sum(Frame_H*To_HH[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_HH = ReName(Sig_HH, 'Sig_HH_Ext')\n",
    "\n",
    "    Frame_H =  tf.signal.frame(Sig_H[:,:,0], To_HL.shape[-1], 1)\n",
    "    Sig_HL = tf.reduce_sum(Frame_H*To_HL[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_HL = ReName(Sig_HL, 'Sig_HL_Ext')\n",
    "\n",
    "\n",
    "    ### Filtering level LH and LL (from Sig_L) -------------------------------------------------------------------\n",
    "    ## Filter generation\n",
    "    To_LH = GenHighFilter(LH_F,  N=FiltLenList[4])\n",
    "    To_LL = GenLowFilter(LL_F,  N=FiltLenList[5])\n",
    "\n",
    "    ## Perform signal filtering level 2\n",
    "    Frame_L =  tf.signal.frame(Sig_L[:,:,0], To_LH.shape[-1], 1)\n",
    "    Sig_LH = tf.reduce_sum(Frame_L*To_LH[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_LH = ReName(Sig_LH, 'Sig_LH_Ext')\n",
    "\n",
    "    Frame_L =  tf.signal.frame(Sig_L[:,:,0], To_LL.shape[-1], 1)\n",
    "    Sig_LL = tf.reduce_sum(Frame_L*To_LL[:,:,::-1], axis=-1, keepdims=True)\n",
    "    Sig_LL = ReName(Sig_LL, 'Sig_LL_Ext')\n",
    "\n",
    "    \n",
    "    return [Flatten()(Sig_HH), Flatten()(Sig_HL), Flatten()(Sig_LH), Flatten()(Sig_LL)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FeatGenerator (Inp_lat):\n",
    "    \n",
    "    #Inp_lat = tf.concat(Inp_lat, axis=-1)\n",
    "    InpZ, FCCommon, FCEach = Inp_lat\n",
    "    HH_F, HL_F, LH_F, LL_F = tf.split(FCEach, 4, axis=1)\n",
    "    \n",
    "   \n",
    "    Dec_Sig_HH = Dense(10, activation='relu')(tf.concat([InpZ, FCCommon, HH_F], axis=-1))\n",
    "    Dec_Sig_HH = Dense(20, activation='relu')(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(30, activation='relu')(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(50, activation='relu')(Dec_Sig_HH)\n",
    "\n",
    "    Dec_Sig_HH = RepeatVector(10 )(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_HH)\n",
    "    Dec_Sig_HH = Dense(40,'tanh')(Dec_Sig_HH)\n",
    "    Sig_HH= Flatten(name='Sig_HH_Gen')(Dec_Sig_HH)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_HL = Dense(10, activation='relu')(tf.concat([InpZ, FCCommon, HL_F], axis=-1))\n",
    "    Dec_Sig_HL = Dense(20, activation='relu')(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(30, activation='relu')(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(50, activation='relu')(Dec_Sig_HL)\n",
    "\n",
    "    Dec_Sig_HL = RepeatVector(10 )(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_HL)\n",
    "    Dec_Sig_HL = Dense(40,'tanh')(Dec_Sig_HL)\n",
    "    Sig_HL= Flatten(name='Sig_HL_Gen')(Dec_Sig_HL)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_LH = Dense(10, activation='relu')(tf.concat([InpZ, FCCommon, LH_F], axis=-1))\n",
    "    Dec_Sig_LH = Dense(20, activation='relu')(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(30, activation='relu')(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(50, activation='relu')(Dec_Sig_LH)\n",
    "\n",
    "    Dec_Sig_LH = RepeatVector(10 )(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_LH)\n",
    "    Dec_Sig_LH = Dense(40,'tanh')(Dec_Sig_LH)\n",
    "    Sig_LH= Flatten(name='Sig_LH_Gen')(Dec_Sig_LH)\n",
    "    \n",
    "    # ---------------------------------------------------------------------- #\n",
    "    \n",
    "    Dec_Sig_LL = Dense(10, activation='relu')(tf.concat([InpZ, FCCommon, LL_F], axis=-1))\n",
    "    Dec_Sig_LL = Dense(20, activation='relu')(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(30, activation='relu')(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(50, activation='relu')(Dec_Sig_LL)\n",
    "\n",
    "    Dec_Sig_LL = RepeatVector(10 )(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(10, return_sequences=True))(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(20, return_sequences=True))(Dec_Sig_LL)\n",
    "    Dec_Sig_LL = Dense(40,'tanh')(Dec_Sig_LL)\n",
    "    Sig_LL= Flatten(name='Sig_LL_Gen')(Dec_Sig_LL)\n",
    "    \n",
    "    return  Sig_HH, Sig_HL, Sig_LH, Sig_LL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Reconstructor(Inps ):\n",
    "    Sig_HH, Sig_HL, Sig_LH, Sig_LL = Inps\n",
    "\n",
    "    ## GRU NET -------------------------------------------------------------------\n",
    "    Dec_Sig_HH = Reshape((-1, 100))(Sig_HH)\n",
    "    Dec_Sig_HL = Reshape((-1, 100))(Sig_HL)\n",
    "    Dec_Sig_LH = Reshape((-1, 100))(Sig_LH)\n",
    "    Dec_Sig_LL = Reshape((-1, 100))(Sig_LL)\n",
    "\n",
    "    Dec_Sig_HH = Bidirectional(GRU(5), name='Dec_Sig_HH')(Dec_Sig_HH)\n",
    "    Dec_Sig_HL = Bidirectional(GRU(5), name='Dec_Sig_HL')(Dec_Sig_HL)\n",
    "    Dec_Sig_LH = Bidirectional(GRU(5), name='Dec_Sig_LH')(Dec_Sig_LH)\n",
    "    Dec_Sig_LL = Bidirectional(GRU(5), name='Dec_Sig_LL')(Dec_Sig_LL)\n",
    "\n",
    "    Decoder = tf.concat([ Dec_Sig_HH, Dec_Sig_HL, Dec_Sig_LH, Dec_Sig_LL], axis=1)\n",
    "    Decoder = RepeatVector((SigDim//100) )(Decoder)\n",
    "    Decoder = Bidirectional(GRU(50, return_sequences=True))(Decoder)\n",
    "    Decoder = Dense(100, activation='relu')(Decoder)\n",
    "    DecOut = Dense(100, activation='sigmoid')(Decoder)\n",
    "    DecOut = Reshape((SigDim,),name='Out')(DecOut)\n",
    "\n",
    "    \n",
    "    return DecOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder - FeatExtractor\n",
    "EncInp, EncOut = Encoder(SigDim=SigDim, LatDim= LatDim, Type = 'Tr', MaskingRate = MaskingRate, NoiseStd = NoiseStd, MaskStd = MaskStd, ReparaStd = ReparaStd, Reparam=True)\n",
    "FeatExtOut = FeatExtractor(EncOut)\n",
    "\n",
    "### Encoder - FeatGenerator - Reconstruction\n",
    "FeatGenOut = FeatGenerator([EncOut[1],EncOut[2][:, :2], EncOut[2][:, 2:]])\n",
    "FeatGenOut = ReName(FeatGenOut, 'FeatGenOut')\n",
    "\n",
    "ReconOut = Reconstructor(FeatExtOut)\n",
    "ReconOut = ReName(ReconOut, 'ReconOut')\n",
    "\n",
    "### Define the total model\n",
    "SigBandRepModel = Model(EncInp, ReconOut)\n",
    "\n",
    "### Weight controller; Apply beta and capacity \n",
    "Capacity_Z = 0.1 # 0.1 0.05\n",
    "Capacity_Fc = 0.6\n",
    "Beta_Z = Lossweight(name='Beta_Z')(FeatGenOut)\n",
    "Beta_Fc = Lossweight(name='Beta_Fc')(FeatGenOut)\n",
    "Beta_Rec = Lossweight(name='Beta_Rec')(FeatGenOut)\n",
    "Beta_Feat = Lossweight(name='Beta_Feat')(FeatGenOut)\n",
    "\n",
    "\n",
    "### Adding the RecLoss; \n",
    "MSE = tf.keras.losses.MeanSquaredError()\n",
    "RecLoss = MSE(ReconOut, EncInp)\n",
    "SigBandRepModel.add_loss(RecLoss * Beta_Rec )\n",
    "SigBandRepModel.add_metric(RecLoss, 'RecLoss')\n",
    "\n",
    "\n",
    "### Adding the FeatRecLoss; It allows connection between the extractor and generator\n",
    "FeatRecLoss= MSE(tf.concat(FeatGenOut, axis=-1), tf.concat(FeatExtOut, axis=-1))\n",
    "SigBandRepModel.add_loss(FeatRecLoss * Beta_Feat )\n",
    "SigBandRepModel.add_metric(FeatRecLoss, 'FeatRecLoss')\n",
    "\n",
    "### KL Divergence for p(Z) vs q(Z)\n",
    "Z_Sampled, Z_Log_Sigma = SigBandRepModel.get_layer('Z_Mean_Tr').output, SigBandRepModel.get_layer('Z_Log_Sigma_Tr').output\n",
    "kl_Loss_Z = 0.5 * tf.reduce_sum( Z_Sampled**2  +  tf.exp(Z_Log_Sigma)- Z_Log_Sigma-1, axis=1)    \n",
    "kl_Loss_Z = tf.reduce_mean(kl_Loss_Z )\n",
    "kl_Loss_Z = Beta_Z * tf.abs(kl_Loss_Z - Capacity_Z)\n",
    "\n",
    "### KL Divergence for p(FCs) vs q(FCs)\n",
    "BernP = 0.5 # hyperparameter\n",
    "FCs = SigBandRepModel.get_layer('FCs').output\n",
    "kl_Loss_FC = tf.math.log(FCs) - tf.math.log(BernP) + tf.math.log(1-FCs) - tf.math.log(1-BernP) \n",
    "kl_Loss_FC = tf.reduce_mean(-kl_Loss_FC )\n",
    "kl_Loss_FC = Beta_Fc * tf.abs(kl_Loss_FC - Capacity_Fc)\n",
    "\n",
    "SigBandRepModel.add_loss(kl_Loss_Z )\n",
    "SigBandRepModel.add_metric(kl_Loss_Z, 'kl_Loss_Z')\n",
    "\n",
    "SigBandRepModel.add_loss(kl_Loss_FC )\n",
    "SigBandRepModel.add_metric(kl_Loss_FC, 'kl_Loss_FC')\n",
    "\n",
    "## Model Compile\n",
    "SigBandRepModel.compile(optimizer='adam') \n",
    "\n",
    "### Loss and KLD_Beta controller\n",
    "KLD_Beta_Z = KLAnneal(TargetLossName=['val_FeatRecLoss', 'val_RecLoss'], Threshold=0.001, BetaName='Beta_Z',  MaxBeta=0.1 , MinBeta=0.1, AnnealEpoch=1, UnderLimit=1e-7, verbose=2)\n",
    "KLD_Beta_Fc = KLAnneal(TargetLossName=['val_FeatRecLoss', 'val_RecLoss'], Threshold=0.001, BetaName='Beta_Fc',  MaxBeta=0.005 , MinBeta=0.005, AnnealEpoch=1, UnderLimit=1e-7, verbose=1)\n",
    "\n",
    "RelLossDic = {'val_RecLoss':'Beta_Rec', 'val_FeatRecLoss':'Beta_Feat', 'val_kl_Loss_Z':'Beta_Z', 'val_kl_Loss_FC':'Beta_Fc'}\n",
    "ScalingDic = {'val_RecLoss':100., 'val_FeatRecLoss':100., 'val_kl_Loss_Z':0.1, 'val_kl_Loss_FC':0.1}\n",
    "MinLimit = {'Beta_Rec':1., 'Beta_Feat':1., 'Beta_Z':0.01, 'Beta_Fc':0.01}\n",
    "MaxLimit = {'Beta_Rec':500., 'Beta_Feat':500., 'Beta_Z':0.1, 'Beta_Fc':0.05}\n",
    "RelLoss = RelLossWeight(BetaList=RelLossDic, LossScaling= ScalingDic, MinLimit= MinLimit, MaxLimit = MaxLimit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      " 6/92 [>.............................] - ETA: 47s - loss: 0.0000e+00 - RecLoss: 0.0440 - FeatRecLoss: 0.0306 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2263s vs `on_train_batch_end` time: 0.2734s). Check your callbacks.\n",
      "92/92 [==============================] - 107s 705ms/step - loss: 0.0000e+00 - RecLoss: 0.0441 - FeatRecLoss: 0.0305 - kl_Loss_Z: 0.0000e+00 - kl_Loss_FC: 0.0000e+00 - val_loss: 0.0000e+00 - val_RecLoss: 0.0441 - val_FeatRecLoss: 0.0306 - val_kl_Loss_Z: 0.0000e+00 - val_kl_Loss_FC: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_FeatRecLoss improved from inf to 0.03056, saving model to ./Results\\SigBandRepModel_ExtRec_0302.hdf5\n",
      "{'val_RecLoss': 44088099.15184975, 'val_FeatRecLoss': 30558494.850993156, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 0.1}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  500.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 2/700\n",
      "92/92 [==============================] - 54s 586ms/step - loss: 16.4913 - RecLoss: 0.0139 - FeatRecLoss: 0.0055 - kl_Loss_Z: 0.0470 - kl_Loss_FC: 0.0018 - val_loss: 5.4956 - val_RecLoss: 0.0083 - val_FeatRecLoss: 0.0026 - val_kl_Loss_Z: 0.0154 - val_kl_Loss_FC: 0.0069\n",
      "\n",
      "Epoch 00002: val_FeatRecLoss improved from 0.03056 to 0.00261, saving model to ./Results\\SigBandRepModel_ExtRec_0302.hdf5\n",
      "{'val_RecLoss': 320.21453516115565, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.5923402388655677, 'val_kl_Loss_FC': 0.2639727204857072}\n",
      "Beta_Rec :  320.21454\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 3/700\n",
      "92/92 [==============================] - 54s 584ms/step - loss: 2.8957 - RecLoss: 0.0081 - FeatRecLoss: 0.0025 - kl_Loss_Z: 0.0024 - kl_Loss_FC: 0.0122 - val_loss: 2.8039 - val_RecLoss: 0.0079 - val_FeatRecLoss: 0.0024 - val_kl_Loss_Z: 6.1499e-04 - val_kl_Loss_FC: 0.0136\n",
      "\n",
      "Epoch 00003: val_FeatRecLoss improved from 0.00261 to 0.00245, saving model to ./Results\\SigBandRepModel_ExtRec_0302.hdf5\n",
      "{'val_RecLoss': 1292.3674691662359, 'val_FeatRecLoss': 397.8754519330796, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 2.2060479977799328}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  397.87546\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 4/700\n",
      "92/92 [==============================] - 54s 584ms/step - loss: 4.9194 - RecLoss: 0.0077 - FeatRecLoss: 0.0023 - kl_Loss_Z: 0.0083 - kl_Loss_FC: 0.0257 - val_loss: 4.6221 - val_RecLoss: 0.0074 - val_FeatRecLoss: 0.0022 - val_kl_Loss_Z: 0.0081 - val_kl_Loss_FC: 0.0301\n",
      "\n",
      "Epoch 00004: val_FeatRecLoss improved from 0.00245 to 0.00217, saving model to ./Results\\SigBandRepModel_ExtRec_0302.hdf5\n",
      "{'val_RecLoss': 343.3606846063779, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.37144909903847534, 'val_kl_Loss_FC': 1.3884258497940252}\n",
      "Beta_Rec :  343.3607\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 5/700\n",
      "92/92 [==============================] - 54s 586ms/step - loss: 2.7366 - RecLoss: 0.0070 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0031 - kl_Loss_FC: 0.0184 - val_loss: 2.5080 - val_RecLoss: 0.0066 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 0.0011 - val_kl_Loss_FC: 0.0131\n",
      "\n",
      "Epoch 00005: val_FeatRecLoss did not improve from 0.00217\n",
      "{'val_RecLoss': 607.4274024066709, 'val_FeatRecLoss': 212.39254247397707, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 1.209396165246487}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  212.39255\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 6/700\n",
      "92/92 [==============================] - 54s 590ms/step - loss: 3.7299 - RecLoss: 0.0063 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0027 - kl_Loss_FC: 0.0221 - val_loss: 3.4306 - val_RecLoss: 0.0059 - val_FeatRecLoss: 0.0022 - val_kl_Loss_Z: 0.0030 - val_kl_Loss_FC: 0.0244\n",
      "\n",
      "Epoch 00006: val_FeatRecLoss did not improve from 0.00217\n",
      "{'val_RecLoss': 268.9947999823132, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.1376724846559257, 'val_kl_Loss_FC': 1.1156526339173698}\n",
      "Beta_Rec :  268.9948\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 7/700\n",
      "92/92 [==============================] - 53s 581ms/step - loss: 1.7925 - RecLoss: 0.0057 - FeatRecLoss: 0.0023 - kl_Loss_Z: 0.0011 - kl_Loss_FC: 0.0164 - val_loss: 1.7086 - val_RecLoss: 0.0054 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 7.0942e-04 - val_kl_Loss_FC: 0.0126\n",
      "\n",
      "Epoch 00007: val_FeatRecLoss did not improve from 0.00217\n",
      "{'val_RecLoss': 767.8911713767252, 'val_FeatRecLoss': 324.15128199994814, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 1.772906598435994}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  324.15128\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 8/700\n",
      "92/92 [==============================] - 56s 606ms/step - loss: 3.4183 - RecLoss: 0.0053 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0040 - kl_Loss_FC: 0.0275 - val_loss: 3.2402 - val_RecLoss: 0.0050 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0050 - val_kl_Loss_FC: 0.0302\n",
      "\n",
      "Epoch 00008: val_FeatRecLoss improved from 0.00217 to 0.00213, saving model to ./Results\\SigBandRepModel_ExtRec_0302.hdf5\n",
      "{'val_RecLoss': 236.0425623157565, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.23554004074640988, 'val_kl_Loss_FC': 1.4168789631413512}\n",
      "Beta_Rec :  236.04256\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 9/700\n",
      "92/92 [==============================] - 54s 585ms/step - loss: 1.4145 - RecLoss: 0.0049 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0016 - kl_Loss_FC: 0.0181 - val_loss: 1.3676 - val_RecLoss: 0.0048 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 3.4200e-04 - val_kl_Loss_FC: 0.0132\n",
      "\n",
      "Epoch 00009: val_FeatRecLoss did not improve from 0.00213\n",
      "{'val_RecLoss': 1394.7693224675613, 'val_FeatRecLoss': 666.9465656520442, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 3.871949550102775}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  500.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 10/700\n",
      "92/92 [==============================] - 54s 585ms/step - loss: 3.4868 - RecLoss: 0.0046 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0072 - kl_Loss_FC: 0.0315 - val_loss: 3.2644 - val_RecLoss: 0.0044 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0085 - val_kl_Loss_FC: 0.0347\n",
      "\n",
      "Epoch 00010: val_FeatRecLoss improved from 0.00213 to 0.00209, saving model to ./Results\\SigBandRepModel_ExtRec_0302.hdf5\n",
      "{'val_RecLoss': 208.0943823127252, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.407049745460376, 'val_kl_Loss_FC': 1.6611059000636907}\n",
      "Beta_Rec :  208.09438\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 11/700\n",
      "92/92 [==============================] - 54s 587ms/step - loss: 1.1366 - RecLoss: 0.0042 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0029 - kl_Loss_FC: 0.0200 - val_loss: 1.0928 - val_RecLoss: 0.0041 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 6.2140e-04 - val_kl_Loss_FC: 0.0142\n",
      "\n",
      "Epoch 00011: val_FeatRecLoss did not improve from 0.00209\n",
      "{'val_RecLoss': 658.610608558255, 'val_FeatRecLoss': 364.279535916558, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 2.2852138926779406}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  364.27954\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 12/700\n",
      "92/92 [==============================] - 54s 588ms/step - loss: 2.8831 - RecLoss: 0.0040 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0049 - kl_Loss_FC: 0.0292 - val_loss: 2.6900 - val_RecLoss: 0.0038 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0063 - val_kl_Loss_FC: 0.0316\n",
      "\n",
      "Epoch 00012: val_FeatRecLoss did not improve from 0.00209\n",
      "{'val_RecLoss': 178.88572081161564, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.2986596281647484, 'val_kl_Loss_FC': 1.5000863348020086}\n",
      "Beta_Rec :  178.88573\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 13/700\n",
      "92/92 [==============================] - 54s 588ms/step - loss: 0.9100 - RecLoss: 0.0037 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0020 - kl_Loss_FC: 0.0188 - val_loss: 0.8867 - val_RecLoss: 0.0036 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 4.1936e-04 - val_kl_Loss_FC: 0.0138\n",
      "\n",
      "Epoch 00013: val_FeatRecLoss did not improve from 0.00209\n",
      "{'val_RecLoss': 860.7627447707689, 'val_FeatRecLoss': 540.572088073234, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 3.2969117876182104}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  500.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 14/700\n",
      "92/92 [==============================] - 54s 587ms/step - loss: 2.8960 - RecLoss: 0.0035 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0069 - kl_Loss_FC: 0.0325 - val_loss: 2.7719 - val_RecLoss: 0.0034 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0088 - val_kl_Loss_FC: 0.0356\n",
      "\n",
      "Epoch 00014: val_FeatRecLoss improved from 0.00209 to 0.00208, saving model to ./Results\\SigBandRepModel_ExtRec_0302.hdf5\n",
      "{'val_RecLoss': 161.93719417466178, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.42249926436456753, 'val_kl_Loss_FC': 1.7083492162016887}\n",
      "Beta_Rec :  161.9372\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 15/700\n",
      "92/92 [==============================] - 54s 585ms/step - loss: 0.7856 - RecLoss: 0.0033 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0026 - kl_Loss_FC: 0.0206 - val_loss: 0.7652 - val_RecLoss: 0.0032 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 6.9865e-04 - val_kl_Loss_FC: 0.0148\n",
      "\n",
      "Epoch 00015: val_FeatRecLoss did not improve from 0.00208\n",
      "{'val_RecLoss': 463.71886265091075, 'val_FeatRecLoss': 322.21863786980157, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 2.114404753295651}\n",
      "Beta_Rec :  463.71887\n",
      "Beta_Feat :  322.21863\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 16/700\n",
      "92/92 [==============================] - 54s 584ms/step - loss: 2.2170 - RecLoss: 0.0032 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0044 - kl_Loss_FC: 0.0274 - val_loss: 2.1373 - val_RecLoss: 0.0031 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0056 - val_kl_Loss_FC: 0.0306\n",
      "\n",
      "Epoch 00016: val_FeatRecLoss did not improve from 0.00208\n",
      "{'val_RecLoss': 144.6997971946247, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.26626364610737174, 'val_kl_Loss_FC': 1.4459562307565479}\n",
      "Beta_Rec :  144.6998\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 17/700\n",
      "92/92 [==============================] - 54s 585ms/step - loss: 0.6815 - RecLoss: 0.0030 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0016 - kl_Loss_FC: 0.0173 - val_loss: 0.6693 - val_RecLoss: 0.0030 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 3.7894e-04 - val_kl_Loss_FC: 0.0124\n",
      "\n",
      "Epoch 00017: val_FeatRecLoss did not improve from 0.00208\n",
      "{'val_RecLoss': 782.3146320866988, 'val_FeatRecLoss': 600.5046752156543, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 3.283894393773394}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  500.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 18/700\n",
      "92/92 [==============================] - 54s 588ms/step - loss: 2.6332 - RecLoss: 0.0029 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0092 - kl_Loss_FC: 0.0314 - val_loss: 2.4983 - val_RecLoss: 0.0028 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0094 - val_kl_Loss_FC: 0.0351\n",
      "\n",
      "Epoch 00018: val_FeatRecLoss did not improve from 0.00208\n",
      "{'val_RecLoss': 135.40887990468974, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.4529964995799965, 'val_kl_Loss_FC': 1.6856746810771257}\n",
      "Beta_Rec :  135.40887\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 19/700\n",
      "92/92 [==============================] - 54s 587ms/step - loss: 0.6246 - RecLoss: 0.0028 - FeatRecLoss: 0.0022 - kl_Loss_Z: 0.0032 - kl_Loss_FC: 0.0189 - val_loss: 0.6116 - val_RecLoss: 0.0027 - val_FeatRecLoss: 0.0023 - val_kl_Loss_Z: 4.6352e-04 - val_kl_Loss_FC: 0.0131\n",
      "\n",
      "Epoch 00019: val_FeatRecLoss did not improve from 0.00208\n",
      "{'val_RecLoss': 590.8679017064887, 'val_FeatRecLoss': 489.98946661489555, 'val_kl_Loss_Z': 0.1, 'val_kl_Loss_FC': 2.829392815899836}\n",
      "Beta_Rec :  500.0\n",
      "Beta_Feat :  489.98947\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 20/700\n",
      "92/92 [==============================] - 54s 587ms/step - loss: 2.4565 - RecLoss: 0.0027 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0067 - kl_Loss_FC: 0.0332 - val_loss: 2.3832 - val_RecLoss: 0.0026 - val_FeatRecLoss: 0.0021 - val_kl_Loss_Z: 0.0088 - val_kl_Loss_FC: 0.0357\n",
      "\n",
      "Epoch 00020: val_FeatRecLoss improved from 0.00208 to 0.00208, saving model to ./Results\\SigBandRepModel_ExtRec_0302.hdf5\n",
      "{'val_RecLoss': 126.64381227912858, 'val_FeatRecLoss': 100.0, 'val_kl_Loss_Z': 0.4218226189553651, 'val_kl_Loss_FC': 1.7157126653988173}\n",
      "Beta_Rec :  126.643814\n",
      "Beta_Feat :  100.0\n",
      "Beta_Z :  0.1\n",
      "Beta_Fc :  0.05\n",
      "Epoch 21/700\n",
      "34/92 [==========>...................] - ETA: 32s - loss: 0.5832 - RecLoss: 0.0026 - FeatRecLoss: 0.0021 - kl_Loss_Z: 0.0058 - kl_Loss_FC: 0.0284"
     ]
    }
   ],
   "source": [
    "#SigBandRepModel.load_weights(ModelSaveSameName)\n",
    "ModelSave = ModelCheckpoint(filepath=ModelSaveSameName, monitor='val_FeatRecLoss', verbose=1, save_best_only=True )\n",
    "\n",
    "SigBandRepModel.fit(DATA[:], batch_size=3500, epochs=700, shuffle=True, validation_split=0.2, callbacks=[EarlyStop, ModelSave, RelLoss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range (5000):\n",
    "\n",
    "    FCs = 0.9\n",
    "    FCs = tf.clip_by_value(FCs, 1e-7, 1-1e-7)\n",
    "    Epsilon = tf.random.uniform(shape=(1, 1))\n",
    "    Epsilon = tf.clip_by_value(Epsilon, 1e-7, 1-1e-7)\n",
    "\n",
    "    LogEps = tf.math.log(Epsilon)\n",
    "    LogNegEps = tf.math.log(1 - Epsilon)\n",
    "\n",
    "    LogTheta = tf.math.log(FCs)\n",
    "    LogNegTheta = tf.math.log(1-FCs)\n",
    "\n",
    "\n",
    "    res.append(tf.math.sigmoid(LogEps - LogNegEps + LogTheta - LogNegTheta).numpy())\n",
    "    \n",
    "plt.hist(np.concatenate(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
